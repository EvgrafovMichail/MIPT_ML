{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whiAANkwhw-I"
      },
      "source": [
        "# MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccGClM6h3jh"
      },
      "source": [
        "Когда Инокетений Вальдемарович Носочков приходит на работу, то с вероятностью 50% он хочет пойти в бар, с 10% – покушац, с 20% – спать. Если он ничего не хочет, то он продолжает работать. Когда Кеша пьет в Баре, то в 10% посещений он возвращается на работу и с вероятностью 30% идет спать, но в остальное время продолжает пить. Когда он просыпается, то с вероятностью 40% идет покушац и с вероятностью 60% идет в бар пить дальше. Если вдруг г-н Носочков поел, то с вероятностью 80% он начинает работать, а если не срослось с работой, то он начинает хотеть спать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS-kwBSRh3mX"
      },
      "source": [
        "Определите вероятности, что наш герой прямо сейчас работает, пьет в баре, спит или ест, при условии, что если Инокетений чего-то хочет, то делает это."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJRO5-9WvM_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuXufW6hAlo-"
      },
      "source": [
        "# Practice: gym interface and crossentropy method\n",
        "\n",
        "_Reference:_ This notebook is based on Practical RL [week01](https://github.com/yandexdataschool/Practical_RL/tree/master/week01_intro)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw_mjq4C9i-N"
      },
      "source": [
        "import sys, os\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ[\"DISPLAY\"] = \":1\"\n",
        "    %env DISPLAY = : 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftOrVNgg9i-O"
      },
      "source": [
        "## OpenAI Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6tdsNtA9hl"
      },
      "source": [
        "We're gonna spend several next weeks learning algorithms that solve decision processes. We are then in need of some interesting decision problems to test our algorithms.\n",
        "\n",
        "That's where OpenAI Gym comes into play. It's a Python library that wraps many classical decision problems including robot control, videogames and board games.\n",
        "\n",
        "So here's how it works:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "gym.__version__"
      ],
      "metadata": {
        "id": "NI0LrNlkY3X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgUa6Yz89i-Q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env.reset()\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "plt.imshow(env.render(\"rgb_array\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRTK8yeU9i-R"
      },
      "source": [
        "### Gym interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMWiCMVIVPTI"
      },
      "source": [
        "The three main methods of an environment are\n",
        "* `reset()`: reset environment to the initial state, and _return it_\n",
        "* `render()`: show current environment state (a more colorful version)\n",
        "* `step(a)`: commit action `a` and return `(new_state, reward, is_done, info)`\n",
        " * `new_state`: the new state right after committing the action `a`\n",
        " * `reward`: a number representing your reward for committing action `a`\n",
        " * `is_done`: True if the MDP has just finished, False if still in progress\n",
        " * `info`: some auxiliary stuff about what just happened. For now, ignore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhGD7yXr9i-R"
      },
      "source": [
        "state = env.reset()\n",
        "print(\"initial state:\", state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9WYWMRvrl5r"
      },
      "source": [
        "In MountainCar, observation is just two numbers: car position and velocity.\n",
        "\n",
        "Let's take action 2, which stands for \"go right\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSs_w9Lu9i-S"
      },
      "source": [
        "print(\"taking action 2 (right)\")\n",
        "new_state, reward, is_done, _ = env.step(2)\n",
        "\n",
        "print(\"new state:\", new_state)\n",
        "print(\"reward:\", reward)\n",
        "print(\"is game over?:\", is_done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1CLfCf6rpx8"
      },
      "source": [
        "As you can see, the car has moved to the right slightly (around 0.0005)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS56PCMa9i-S"
      },
      "source": [
        "### Play with it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrL9_D56VXSI"
      },
      "source": [
        "Below is the code that drives the car to the right. However, if you simply use the default policy, the car will not reach the flag at the far right due to gravity.\n",
        "\n",
        "__Your task__ is to fix it. Find a strategy that reaches the flag. \n",
        "\n",
        "You are not required to build any sophisticated algorithms for now, and you definitely don't need to know any reinforcement learning for this. Feel free to hard-code :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CaEYOrf9i-T"
      },
      "source": [
        "actions = {\"left\": 0, \"stop\": 1, \"right\": 2}\n",
        "\n",
        "def policy(state, time_step):\n",
        "    # Write the code for your policy here. You can use the current state\n",
        "    # (a tuple of position and velocity), the current time step, or both,\n",
        "    # if you want.\n",
        "    \n",
        "    # Ваш код здесь\n",
        "\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k02kqprA9i-T"
      },
      "source": [
        "from IPython.display import clear_output, display\n",
        "\n",
        "state = env.reset()\n",
        "time_limit = 250\n",
        "is_done = 0\n",
        "for time_step in range(time_limit):\n",
        "    # Choose action based on your policy.\n",
        "\n",
        "    # Pass the action to the environment.\n",
        "    \n",
        "    # We don't do anything with reward here because MountainCar is a very\n",
        "    # simple environment, and reward is a constant -1 (meaning that your\n",
        "    # goal is to end the episode as quickly as possible).\n",
        "    action = policy(state, time_step)\n",
        "    state, reward, is_done, _ = env.step(action)\n",
        "\n",
        "    # Draw game image on display.\n",
        "    clear_output(wait=True)\n",
        "    plt.imshow(env.render(\"rgb_array\"))\n",
        "    plt.show()\n",
        "\n",
        "    if is_done:\n",
        "          print(\"Well done!\")\n",
        "          break\n",
        "if not is_done:\n",
        "    print(\"Time limit exceeded. Try again.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LYwulaIBZeH"
      },
      "source": [
        "## Crossentropy method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7eHzR_9Bm2K"
      },
      "source": [
        "Now that we know how does the `gym` work, let's try and solve a more complicated problem using the crossentropy method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boirRXNq--s-"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU66aOiPdVVY"
      },
      "source": [
        "As `Taxi-v3` is a much more sophisticated environment, it presents us with more possible states and actions at our disposal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9l4J5bpB04k"
      },
      "source": [
        "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "print(f\"n_states={n_states}, n_actions={n_actions}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxLN1ZxmdxcI"
      },
      "source": [
        "That's definitely a lot. Way too much to hard-code as we did with previous problem. Let's use the crossentropy method on this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FExaEKW-Vf2j"
      },
      "source": [
        "### Create stochastic policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3B63b1fVl6C"
      },
      "source": [
        "This time our policy should be a probability distribution.\n",
        "\n",
        "```policy[s, a] = P(take action a | in state s)```\n",
        "\n",
        "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
        "\n",
        "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_TzXJVHVlY3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_policy(n_states, n_actions):\n",
        "    # Create an array to store action probabilities\n",
        "    \n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99xkrpd0pbX"
      },
      "source": [
        "policy = initialize_policy(n_states, n_actions)\n",
        "policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THBVPqvOW2OA"
      },
      "source": [
        "### Play the game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOgE_1xMXQ61"
      },
      "source": [
        "Let's play the game just like before, however this time we will also record states, actions and rewards to use them in training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QBRMMZeW38F"
      },
      "source": [
        "def generate_session(env, policy, time_limit=10**4):\n",
        "    state = env.reset()\n",
        "    states, actions = [], []\n",
        "    total_reward = 0.\n",
        "    for _ in range(time_limit):\n",
        "        # Choose action based on policy and take it.\n",
        "        # Record information we just got from the environment.\n",
        "        # Ваш код здесь\n",
        "\n",
        "\n",
        "\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "    return states, actions, total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFF98gn8Xnme"
      },
      "source": [
        "states, actions, reward = generate_session(env, policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3KlOA04YCpj"
      },
      "source": [
        "Let's see the initial reward distribution for our \"naive\" policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP5VOP15X59V"
      },
      "source": [
        "sample_rewards = [generate_session(env, policy, time_limit=1000)[2] for _ in range(200)]\n",
        "plt.hist(sample_rewards, bins=20)\n",
        "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color=\"green\")\n",
        "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color=\"red\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(sample_rewards, 90)"
      ],
      "metadata": {
        "id": "dkdNsDUG_jtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Hbt1n1YK9G"
      },
      "source": [
        "### Crossentropy method step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5WSk58tYUpp"
      },
      "source": [
        "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile.\n",
        "\n",
        "    Compute minimum reward for session to be elite and choose elite states\n",
        "    and actions based on this threshold.\n",
        "\n",
        "    Note that states_batch and actions_batch are both 2d lists, i.e. lists\n",
        "    containing lists of states and actions from each session in batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    elite_states = []\n",
        "    elite_actions = []\n",
        "    # Ваш код здесь\n",
        "\n",
        "\n",
        "    return elite_states, elite_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMQ00-p6UN3v"
      },
      "source": [
        "def get_new_policy(elite_states, elite_actions):\n",
        "    \"\"\"\n",
        "    Given a list of elite states/actions from select_elites, return a new\n",
        "    policy where each action probability is proportional to\n",
        "\n",
        "        policy[s, a] ~ #[occurrences of s and a in elite states/actions]\n",
        "\n",
        "    Don't forget to normalize the policy to get valid probabilities.\n",
        "    For states that you never visited, use a uniform distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    new_policy = np.ones([n_states, n_actions])\n",
        "\n",
        "    # Set probabilities for actions given elite states & actions.\n",
        "    # Ваш код здесь\n",
        "\n",
        "    \n",
        "    return new_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4SYwLlztaLv"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3E8Jf0ttdUw"
      },
      "source": [
        "Generate sessions, select N best and fit to those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8ds_27BtEKV"
      },
      "source": [
        "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    threshold = np.percentile(rewards_batch, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "    \n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "\n",
        "    mean_rewards = [mean_reward for mean_reward, threshold in log]\n",
        "    reward_thresholds = [threshold for mean_reward, threshold in log]\n",
        "    plt.plot(mean_rewards, label=\"Mean rewards\")\n",
        "    plt.plot(reward_thresholds, label=\"Reward thresholds\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(rewards_batch, range=reward_range)\n",
        "    plt.vlines(\n",
        "        [np.percentile(rewards_batch, percentile)],\n",
        "        ymin=[0],\n",
        "        ymax=[100],\n",
        "        label=\"percentile\",\n",
        "        color=\"red\",\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    clear_output(wait=True)\n",
        "    print(f\"mean reward = {mean_reward:.3f}, threshold={threshold:.3f}\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uh8JpOp0yXL"
      },
      "source": [
        "# reset policy\n",
        "policy = initialize_policy(n_states, n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWwgfYHB00dE"
      },
      "source": [
        "n_sessions = 250     # sample this many sessions\n",
        "percentile = 70      # take this percent of session with highest rewards\n",
        "learning_rate = 0.8  # how quickly the policy is updated, on a scale from 0 to 1\n",
        "\n",
        "log = []\n",
        "\n",
        "for i in range(40):\n",
        "    # Generate a list of n_sessions new sessions, select elites and compute\n",
        "    # new policy based on them. After that update the existing policy wrt\n",
        "    # learning rate.\n",
        "    sessions = [generate_session(env, policy)  for _ in range(n_sessions)]\n",
        "    states_batch = [session_states for session_states, session_actions, session_reward in  sessions]\n",
        "    actions_batch = [session_actions for session_states, session_actions, session_reward in  sessions]\n",
        "    rewards_batch = [session_reward for session_states, session_actions, session_reward in  sessions]\n",
        "\n",
        "    # Ваш код здесь\n",
        "    \n",
        "\n",
        "    # display results on chart\n",
        "    show_progress(rewards_batch, log, percentile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlIrztK22HqI"
      },
      "source": [
        "### Analysing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hehsmk982J70"
      },
      "source": [
        "You may have noticed that the taxi problem quickly converges from very little values to a near-optimal score and then descends back. This is caused (at least in part) by the innate randomness of the environment. Namely, the starting points of passenger/driver change from episode to episode.\n",
        "\n",
        "In such case if crossentropy policy failed to learn how to win from one distinct starting point, it will simply discard it because no sessions from that starting point will make it into the \"elites\".\n",
        "\n",
        "To mitigate that problem, you can either reduce the threshold for elite sessions (duct tape way) or change the way you evaluate strategy (theoretically correct way). For each starting state, you can sample an action randomly, and then evaluate this action by running several games starting from it and averaging the total reward. Choosing elite sessions with this kind of sampling (where each session's reward is counted as the average of the rewards of all sessions with the same starting state and action) should improve the performance of your policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTClEj9c3JrE"
      },
      "source": [
        "## Deeging deeper: approximate crossentropy with neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZFlyLJf3TAA"
      },
      "source": [
        "In this section we'll extend your CEM implementation with neural networks! You will train a multi-layer neural network to solve simple continuous state space games.\n",
        "\n",
        "![img](https://watanimg.elwatannews.com/old_news_images/large/249765_Large_20140709045740_11.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS6CtVHR1mRR"
      },
      "source": [
        "# .env is to remove auto-assigned time limit wrapper\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "print(\"state vector dim =\", state_dim)\n",
        "print(\"n_actions =\", n_actions)\n",
        "plt.imshow(env.render(\"rgb_array\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiAvTBy8r9qd"
      },
      "source": [
        "Here, just like in a `MountainCar-v0`, we will be controlling a cart, which we can move right or left. However our goal here is different. In this environment we want to keep pole attached to the top of our cart from falling as long as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRr7zCW4F9B"
      },
      "source": [
        "### Neural Network Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXRmz-B4I7L"
      },
      "source": [
        "For this assignment we'll utilize the simplified neural network implementation from [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Here's what you'll need:\n",
        "* `agent.partial_fit(states, actions)` - make a single training pass over the data to increase the probability of provided `actions` in provided `states`\n",
        "* `agent.predict_proba(states)` - predict probabilities of all actions, a matrix of shape `[len(states), n_actions] `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3-zE4Yp3uV8"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "agent = MLPClassifier(\n",
        "    hidden_layer_sizes=(20, 20),\n",
        "    activation=\"tanh\",\n",
        ")\n",
        "\n",
        "# initialize agent to the dimension of state space and number of actions\n",
        "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO3tM6wFtZr_"
      },
      "source": [
        "Despite the apparent differences, you will find the training procedure for such agent to be very similar to the one we used in the previous part. We won't even need to rewrite most of our helper functions at all! However, one thing that has changed is the way we get actions' probabilities. So let's adapt our `generate_session` function to this new agent-based policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glqypTJL6PmE"
      },
      "source": [
        "def generate_session(env, agent, time_limit=10**4):\n",
        "    state = env.reset()\n",
        "    states, actions = [], []\n",
        "    total_reward = 0.\n",
        "    for _ in range(time_limit):\n",
        "        # Use agent to predict a vector of action probabilities for current \n",
        "        # state and use the probabilities you predicted to pick an action.\n",
        "        # Sample actions, don't just take the most likely one!\n",
        "\n",
        "        action =  np.random.choice # Ваш код здесь\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Record information we just got from the environment.\n",
        "        \n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "    return states, actions, total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6-YrqLP6L-1"
      },
      "source": [
        "states, actions, reward = generate_session(env, agent, time_limit=100)\n",
        "print(\"states:\", np.stack(states))\n",
        "print(\"actions:\", actions)\n",
        "print(\"reward:\", reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cj1aiN7cH-"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFpPGkXL7nCa"
      },
      "source": [
        "n_sessions = 100\n",
        "percentile = 70\n",
        "\n",
        "log = []\n",
        "\n",
        "for _ in range(100):\n",
        "    # Generate new sessions, select elites and update our agent.\n",
        "\n",
        "    sessions = [generate_session(env, agent)  for _ in range(n_sessions)]\n",
        "    states_batch = [session_states for session_states, session_actions, session_reward in  sessions]\n",
        "    actions_batch = [session_actions for session_states, session_actions, session_reward in  sessions]\n",
        "    rewards_batch = [session_reward for session_states, session_actions, session_reward in  sessions]\n",
        "\n",
        "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
        "\n",
        "    # Ваш код здесь\n",
        "\n",
        "    show_progress(\n",
        "        rewards_batch, \n",
        "        log, \n",
        "        percentile, \n",
        "        reward_range=[0, np.max(rewards_batch)]\n",
        "    )\n",
        "\n",
        "    if np.mean(rewards_batch) > 190:\n",
        "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3QsLwz38h9h"
      },
      "source": [
        "### Analysing the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output, display\n",
        "total_reward = 0\n",
        "total_steps = 0\n",
        "# Ваш код здесь"
      ],
      "metadata": {
        "id": "WE8xZc7jTGXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
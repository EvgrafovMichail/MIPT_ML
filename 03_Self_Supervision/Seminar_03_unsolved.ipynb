{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNrs7vZwSNTS"
   },
   "source": [
    "# Семинар № 3 - Self-Supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DW6a6Y8hWtT",
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pythin\n",
    "!pip install albumentations\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYyIZ6dphbN8"
   },
   "source": [
    "## <font color='orange'>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQrK-eB3mBAC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIqaJk_s0H3t",
    "outputId": "3fe41b71-e7a8-48f8-db1c-58562100d709"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix all seeds\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCLR - A Simple Framework for Contrastive Learning of Visual Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За последние несколько лет мы стали свидетелями огромного прогресса в обучении с самоконтролем специально для задач, связанных с компьютерным зрением. В то время как область обработки естественного языка извлекала выгоду из достоинств обучения с самоконтролем в течение долгого времени, но это было не так давно, системы компьютерного зрения начали видеть реальное влияние парадигм обучения с самоконтролем. Такие работы, как [MoCo](https://arxiv.org/abs/1911.05722), [PIRL](https://arxiv.org/abs/1912.01991) продемонстрировали, какие преимущества системы с самоконтролем могут принести для решения проблем, связанных с компьютерным зрением.\n",
    "\n",
    "Chen at al. выпустили работу [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf) (SimCLR), в которой представлена более простая, но эффективная структура для обучения моделей на основе компьютерного зрения с самоконтролем.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/5ab5e0c019cdd8129b4450539231f34dc028c0cd64ba5d50db510d1ba2184160/68747470733a2f2f312e62702e626c6f6773706f742e636f6d2f2d2d764834504b704539596f2f586f3461324259657276492f414141414141414146704d2f766146447750584f79416f6b4143385868383532447a4f67457332324e68625877434c63424741735948512f73313630302f696d616765342e676966\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Оффициальный [репозиторий](https://github.com/google-research/simclr)\n",
    "\n",
    "[Статья](https://arxiv.org/abs/2011.02803)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея самообучения (Self-Supervision) в компьютерном зрении\n",
    "\n",
    "Для обучения моделей Self-Supervision с полностью неразмеченными данными необходимо сначала сформулировать контролируемую задачу обучения (также известную как предварительное текстовое задание) с этими неразмеченными данными. Например, для обучения встраиванию слов в большой корпус, такой как Википедия, вы можете решить задачу предсказания следующего слова по заданной последовательности слов. Итак, как вы могли бы расширить эту идею, когда дело доходит до работы с немаркированными изображениями?\n",
    "\n",
    "Важно отметить, что задача контрастного прогнозирования не должна быть ни слишком простой, ни сложной, и она должна помочь модели развить понимание заданных данных (подумайте о том, как встраивания фиксируют семантические отношения между связанными словами).\n",
    "\n",
    "Вы можете узнать больше об этом из следующей статьи [Self-supervised learning and computer vision](https://www.fast.ai/2020/01/13/self_supervised/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar import load_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\", channels_last=True)\n",
    "\n",
    "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Аугментации с библиотекой ```albumentations``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albumentations — это быстрая и гибкая библиотека для увеличения изображений. Библиотека широко используется в промышленности, исследованиях глубокого обучения, конкурсах по машинному обучению и проектах с открытым исходным кодом. Альбументации написаны на Python и распространяются под лицензией MIT. Исходный код доступен по адресу https://github.com/albumentations-team/albumentations.\n",
    "\n",
    "Гайды доступны на странице [Introduction to image augmentation](https://albumentations.ai/docs/#introduction-to-image-augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательная функция\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# порядок функций аугментации \n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomRotate90()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выберем случайное изображение\n",
    "idx = np.random.choice(tuple(range(len(X_train))), 1)[0]\n",
    "\n",
    "sample_image = (X_train[idx] * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image = transform(image=sample_image)['image']\n",
    "visualize(augmented_image)\n",
    "visualize(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "        A.RandomRotate90(),\n",
    "        A.Flip(),\n",
    "        A.Transpose(),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.RandomBrightnessContrast(),            \n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image = transform(image=sample_image)['image']\n",
    "visualize(augmented_image)\n",
    "visualize(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Generator for Contrastive Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В оригинальной статье предложен следующий подход к аугментации данных, приведенный на рисунке ниже.\n",
    "\n",
    "![transforms](https://i.ibb.co/7JFG4pf/image.png)\n",
    "\n",
    "### Задание № 1\n",
    "Ваша задача создать ```Dataset```, который использует функции аугментаций на основе библиотеки ```albumentations``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform_augment=None):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        assert transform_augment is not None, 'set transform_augment'\n",
    "        self.transform_augment = transform_augment\n",
    "                    \n",
    "    def __len__(self):\n",
    "        # TODO: pass your code\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = self.x_data[item]\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        label = self.y_data[item]\n",
    "        \n",
    "        # TODO: augment images\n",
    "        x1 = # TODO: pass your code\n",
    "        x2 = # TODO: pass your code\n",
    "        \n",
    "        return x1, x2, label, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание № 2\n",
    "\n",
    "Расчитайте среднее значение и дисперсию для функции нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate values for normalizations\n",
    "MEAN = # TODO: pass your code\n",
    "STD = # TODO: pass your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание № 3\n",
    "\n",
    "Задайте преобразования для шага обучения и шага валидации. На валидации не должно быть преобразований, изменяющих исходные данные. \n",
    "\n",
    "Для нормализации воспользуйтесь функцией ```A.Normalize()```, а для преобразования в тензор - ```ToTensorV2()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    # TODO: pass your code\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    # TODO: pass your code\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CLDataset(X_train, y_train, transform_augment=train_transform)\n",
    "valid_dataset = CLDataset(X_val, y_val, transform_augment=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size,       \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=n_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка итераций загрузчика данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, _, _ = next(iter(train_loader))\n",
    "print(x1.shape, x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(x, STD, MEAN):\n",
    "    plt.figure(figsize=[12, 10])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(x, 4, )\n",
    "    grid = grid.numpy().transpose((1, 2, 0))\n",
    "    grid = grid * STD + MEAN\n",
    "    plt.imshow(grid)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(x1, STD, MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим функции для загрузки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для обрезки набора данных\n",
    "\n",
    "def get_cropped_data_idxs(data, crop_coef: float = 1.0):\n",
    "    crop_coef = np.clip(crop_coef, 0, 1)\n",
    "    \n",
    "    init_data_size = len(data)\n",
    "    final_data_size = int(init_data_size * crop_coef)\n",
    "    \n",
    "    random_idxs = np.random.choice(tuple(range(init_data_size)), final_data_size, replace=False)\n",
    "    return random_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(X_train, y_train, X_val, y_val, crop_coef=0.2):\n",
    "    train_idxs = get_cropped_data_idxs(X_train, crop_coef=crop_coef)\n",
    "    train_data = X_train[train_idxs]\n",
    "    train_labels = y_train[train_idxs] \n",
    "\n",
    "    valid_idxs = get_cropped_data_idxs(X_val, crop_coef=crop_coef)\n",
    "    valid_data = X_val[valid_idxs]\n",
    "    valid_labels = y_val[valid_idxs] \n",
    "    \n",
    "    train_dataset = CLDataset(train_data, train_labels, transform_augment=train_transform)\n",
    "    valid_dataset = CLDataset(valid_data, valid_labels, transform_augment=valid_transform)\n",
    "    \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = load_datasets(X_train, y_train, X_val, y_val, crop_coef=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, _, ori = next(iter(train_loader))\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(x1, STD, MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(x2, STD, MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(ori, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRZ-ey1_2mPZ",
    "tags": []
   },
   "source": [
    "# Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя описанный алгоритм загрузки данных, теперь мы можем реализовать SimCLR. На каждой итерации мы получаем для каждого изображения две по-разному дополненные версии, которые мы обозначаем как и. \n",
    "\n",
    "Оба этих изображения закодированы в одномерный вектор признаков, между которыми мы хотим добиться максимального сходства, что сводит его к минимуму по отношению ко всем другим изображениям в пакете. Сеть кодировщика разделена на две части: базовую сеть кодировщика и проекционную головку .\n",
    "\n",
    "Базовая сеть обычно представляет собой глубокую CNN и отвечает за извлечение вектора представления из примеров дополненных данных. В наших экспериментах мы будем использовать общую архитектуру ResNet-18. \n",
    "Проекционная голова (MLP) отображает представление в пространство, где мы применяем контрастивную ошибку, т.е. сравниваем сходства между векторами. \n",
    "Часто выбирается небольшой MLP с нелинейностями, и для простоты мы следуем оригинальной настройке SimCLR, определяя его как двухслойный MLP с активацией ReLU в скрытом слое. Обратите внимание, что в последующей статье, SimCLRv2, авторы упоминают, что большие / более широкие MLP могут значительно повысить производительность. Вот почему мы применяем MLP с вчетверо большими скрытыми измерениями, но более глубокие MLP показали, что они превосходят данный набор данных. Общая настройка визуализирована ниже:\n",
    "\n",
    "![](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/simclr_network_setup.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 use_bias = True,\n",
    "                 use_bn = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, \n",
    "                                self.out_features, \n",
    "                                bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "             self.bn = nn.BatchNorm1d(self.out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYkMcwWYmawC"
   },
   "outputs": [],
   "source": [
    "def l2_norm(input, axis=1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 head_type = 'nonlinear',\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features, self.out_features, use_bias=False, use_bn=True)\n",
    "        elif self.head_type == 'nonlinear':\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features, self.hidden_features, use_bias=True, use_bn=True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features, self.out_features, use_bias=False, use_bn=True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = l2_norm(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class PreModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # pretrained model        \n",
    "        model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*tuple(model.children())[:-1])\n",
    "        \n",
    "        emb_size = tuple(model.children())[-1].in_features\n",
    "        \n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "        self.projector = ProjectionHead(emb_size, 2048, 128)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.encoder(x)\n",
    "        \n",
    "        xp = self.projector(torch.squeeze(out))\n",
    "        \n",
    "        return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "6213f8efc85b41b8b9c23375c416057e",
      "1a700b2fc234400498581d2081c89153",
      "67a457df909442e58782494206d86974",
      "159070707f784605a2336cffb647d02f",
      "a38a8031fa3f45feaa930bc12f2efc28",
      "c3777ffbc9f941e5a52e11a7e2a59ec0",
      "f47647ab20454613b70c4f78c6175c7b",
      "6a7b6912ecb3418da2aa22ef34d9ae5a"
     ]
    },
    "id": "CHJYDr2kFG1J",
    "outputId": "23e6eaef-2fa7-430f-8d2d-bec5cfc29dae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = PreModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4jCRPqKpl_i"
   },
   "outputs": [],
   "source": [
    "x = np.random.random((32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P9kuZf1FbXk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = model(torch.tensor(x, device=device, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2ZXrAypFN1r",
    "outputId": "26a61aca-47c8-44ba-b669-fb07cae2c657"
   },
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEaRSe00ZRIO",
    "tags": []
   },
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zSuiZ5zpl_j",
    "tags": []
   },
   "source": [
    "## Функция ошибки\n",
    "\n",
    "Normalized Temperature-Scaled Cross-Entropy Loss (NT-XEnt loss)\n",
    "\n",
    "Теперь, когда архитектура описана, давайте подробнее рассмотрим, как мы обучаем модель. Как упоминалось ранее, мы хотим максимизировать сходство между представлениями двух дополненных версий одного и того же изображения на рисунке выше, сводя его к минимуму для всех других примеров в пакете. Короче говоря, функция ошибки сравнивает сходство $z_i$ и $z_j$\n",
    "к любому другому представлению в пакете, выполнив softmax над значениями подобия. Ошибка может быть формально записана как:\n",
    "\n",
    "$$\n",
    "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
    "$$\n",
    "\n",
    "где\n",
    "$$\n",
    "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1Ileef6pl_j"
   },
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "        \n",
    "        self.tot_neg = 0\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        We do not sample negative examples explicitly.\n",
    "        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n",
    "        \"\"\"\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        #SimCLR\n",
    "        labels = torch.from_numpy(np.array([0] * N)).reshape(-1).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2XOwfizpl_j",
    "tags": []
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "### LARS \n",
    "\n",
    "Обычный способ ускорить обучение больших сверточных сетей — добавить вычислительные единицы. Затем обучение выполняется с использованием параллельных синхронных данных. С увеличением количества ГПУ увеличивается размер пакета (batch size). Но обучение с большим размером партии часто приводит к снижению точности модели. Авторы публикации [LARS](https://arxiv.org/pdf/1708.03888.pdf) предлагают способ, как этого можно избежать.\n",
    "\n",
    "Реализация [LARS](https://github.com/Spijkervet/SimCLR/blob/cd85c4366d2e6ac1b0a16798b76ac0a2c8a94e58/simclr/modules/lars.py) для ```pytorch```.\n",
    "\n",
    "Однако для упрощения примера мы рассмотрим обучение через Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lars import LARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJPC4wYwpl_l"
   },
   "source": [
    "## Визуализация эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz09Sza8pl_n"
   },
   "outputs": [],
   "source": [
    "def plot_features(model, dataloader, device='cpu'):\n",
    "    feats = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, total=len(dataloader), desc='collect feats')\n",
    "        for x1, x2, label, _ in pbar:\n",
    "            x1 = x1.to(device)\n",
    "            out = model(x1)\n",
    "            out = out.cpu().data.numpy()\n",
    "            feats.append(out)\n",
    "            labels.append(label.cpu().data.numpy())\n",
    "            \n",
    "    feats = np.concatenate(feats)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print('Train TSNE ...')\n",
    "    tsne = TSNE(n_components=2, perplexity=50, verbose=0, n_jobs=4)\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "    \n",
    "    print('Plot labels ...')\n",
    "    num_classes = len(np.unique(labels))\n",
    "    fig = plt.figure(figsize=(6.4 * 2, 4.8 * 1))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        label_idxs = np.argwhere(labels == i)\n",
    "        plt.scatter(x_feats[label_idxs, 1],x_feats[label_idxs, 0])\n",
    "    \n",
    "    plt.legend([str(i) for i in range(num_classes)])\n",
    "    plt.axis('off')\n",
    "    plt.margins(0)\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdumxB3Gc_IG"
   },
   "outputs": [],
   "source": [
    "fig = plot_features(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.canvas.draw()\n",
    "image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "plt.imshow(image_from_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainProcess:\n",
    "    def __init__(self, hyp):\n",
    "        start_time = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "        log_dir = (Path(\"logs\") / start_time).as_posix()\n",
    "        print('Log dir:', log_dir)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        \n",
    "        self.best_loss = 1e100\n",
    "        self.best_acc = 0.0\n",
    "        self.current_epoch = -1\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.hyp = hyp\n",
    "\n",
    "        self.lr_scheduler: Optional[torch.optim.lr_scheduler] = None\n",
    "        self.model: Optional[torch.nn.modules] = None\n",
    "        self.optimizer: Optional[torch.optim] = None\n",
    "        self.criterion: Optional[torch.nn.modules] = None\n",
    "\n",
    "        self.train_loader: Optional[Dataloader] = None\n",
    "        self.valid_loader: Optional[Dataloader] = None\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def _init_data(self):\n",
    "\n",
    "        train_dataset, valid_dataset = load_datasets(X_train, y_train, X_val, y_val, crop_coef=1.4)\n",
    "        print('Train size:', len(train_dataset), 'Valid size:', len(valid_dataset))\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset,\n",
    "                                       batch_size=self.hyp['batch_size'],\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=self.hyp['n_workers'],\n",
    "                                       pin_memory=True,\n",
    "                                       drop_last=True\n",
    "                                      )\n",
    "\n",
    "        self.valid_loader = DataLoader(valid_dataset,\n",
    "                                       batch_size=self.hyp['batch_size'],\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=self.hyp['n_workers'],\n",
    "                                       pin_memory=True,\n",
    "                                       drop_last=True\n",
    "                                      )\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.model = PreModel()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        model_params = [params for params in self.model.parameters() if params.requires_grad]\n",
    "        # self.optimizer = LARS(model_params, lr=0.2, weight_decay=1e-4)\n",
    "        self.optimizer = torch.optim.AdamW(model_params, lr=self.hyp['lr'], weight_decay=self.hyp['weight_decay'])\n",
    "\n",
    "        # \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "        self.warmupscheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda epoch: (epoch + 1) / 10.0)\n",
    "        self.mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            500,\n",
    "            eta_min=0.05,\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "        \n",
    "        self.criterion = SimCLR_Loss(batch_size=self.hyp['batch_size'], \n",
    "                                     temperature=self.hyp['temperature']).to(self.device)\n",
    "\n",
    "    def init_params(self):\n",
    "        self._init_data()\n",
    "        self._init_model()\n",
    "\n",
    "    def save_checkpoint(self, loss_valid, path):\n",
    "        if loss_valid[0] <= self.best_loss:\n",
    "            self.best_loss = loss_valid[0]\n",
    "            self.save_model(path)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.mainscheduler.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        cum_loss = 0.0\n",
    "        proc_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader),\n",
    "                    desc=f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
    "        for idx, (xi, xj, _, _) in pbar:\n",
    "            xi, xj = xi.to(self.device), xj.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                zi = self.model(xi)\n",
    "                zj = self.model(xj)\n",
    "                loss = self.criterion(zi, zj)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            cur_loss = loss.detach().cpu().numpy()\n",
    "            cum_loss += cur_loss\n",
    "\n",
    "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
    "\n",
    "            s = f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, Loss: {proc_loss:4.3f}'\n",
    "            pbar.set_description(s)\n",
    "\n",
    "        cum_loss /= len(self.train_loader)\n",
    "        return [cum_loss]\n",
    "\n",
    "    def valid_step(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        cum_loss = 0.0\n",
    "        proc_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader),\n",
    "                    desc=f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
    "        for idx, (xi, xj, _, _) in pbar:\n",
    "            xi, xj = xi.to(self.device), xj.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                zi = self.model(xi)\n",
    "                zj = self.model(xj)\n",
    "                loss = self.criterion(zi, zj)\n",
    "\n",
    "            cur_loss = loss.detach().cpu().numpy()\n",
    "            cum_loss += cur_loss\n",
    "\n",
    "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
    "\n",
    "            s = f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, Loss: {proc_loss:4.3f}'\n",
    "            pbar.set_description(s)\n",
    "\n",
    "        cum_loss /= len(self.valid_loader)\n",
    "        return [cum_loss]\n",
    "\n",
    "    def run(self):\n",
    "        best_w_path = 'best.pt'\n",
    "        last_w_path = 'last.pt'\n",
    "        \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        for epoch in range(self.hyp['epochs']):\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            loss_train = self.train_step()\n",
    "            train_losses.append(loss_train)\n",
    "                \n",
    "            if epoch < 10:\n",
    "                self.warmupscheduler.step()\n",
    "            else:\n",
    "                self.mainscheduler.step()\n",
    "\n",
    "            lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            loss_valid = self.valid_step()\n",
    "            valid_losses.append(loss_valid)\n",
    "            \n",
    "            self.save_checkpoint(loss_valid, best_w_path)\n",
    "            \n",
    "            self.writer.add_scalar('Train/Loss', loss_train[0], epoch)\n",
    "            self.writer.add_scalar('Valid/Loss', loss_valid[0], epoch)\n",
    "            self.writer.add_scalar('Lr', lr, epoch)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0 or epoch == self.hyp['epochs'] - 1:\n",
    "                fig = plot_features(self.model, self.valid_loader, device=self.device)\n",
    "                fig.canvas.draw()\n",
    "                image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "                image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "                self.writer.add_image('TSNE val embedings', image_from_plot, epoch, dataformats='HWC')\n",
    "\n",
    "        self.save_model(last_w_path)\n",
    "        torch.cuda.empty_cache()\n",
    "        self.writer.close()\n",
    "\n",
    "        return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyp_params.yaml', 'r') as f:\n",
    "    hyps = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    \n",
    "print(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hyps['seed'])\n",
    "\n",
    "trainer = BaseTrainProcess(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Задание 3: Downstream task\n",
    "\n",
    "## Задание 3.1\n",
    "Ваша задача обучить заданную вами модель CNN методом SimCLR для набора данных CIFAR10, а затем использовать обученный энкодер для решения задачи классификации.\n",
    "\n",
    "**Примечание:**\n",
    "- В качестве базового энкодера можете использовать модель из семинара выше (```resnet18```) или определить свою сеть. **Рекомендуется** использовать уже предобученные сети из [torch zoo](https://pytorch.org/vision/stable/models.html).\n",
    "- В качестве обучающего множества для задачи классификации воспользуйетсь ```X_test```, ```y_test```, которые были определены в самом начале ноутбука. Создайте обучающую и валидационную выборку.\n",
    "- В новой CNN для задачи классификации требуется обучить только полносвязные слои после энкодера. Для этого вам потребуется \"заморозить\" веса энкодера и не учить их. Сделать это можно с помощью команды: \n",
    "```python\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False  \n",
    "```\n",
    "- Процесс обучения модели можно взять из предыдущего семинара\n",
    "\n",
    "## Задание 3.2\n",
    "\n",
    "Сравните полученный результат обучения c и без использования метода SimCLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Литература и ссылки\n",
    "- Документация [Albumentations](https://albumentations.ai/)\n",
    "- Предобученные модели [torchvision](https://pytorch.org/vision/stable/models.html)\n",
    "- [LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS](https://arxiv.org/pdf/1708.03888.pdf)\n",
    "- [SimCLR](https://arxiv.org/pdf/2002.05709.pdf)\n",
    "- [Self-supervised learning and computer vision](https://www.fast.ai/2020/01/13/self_supervised/)\n",
    "- [Pytorch SimCLR](https://github.com/sthalles/SimCLR)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NwHbQkep1MLx",
    "uxVU4yN2fmNf",
    "ziLMBZNX5M38",
    "zB7FcvNU5TyH",
    "yS0S5_Qr5fKm",
    "K6kf2F-K5pw7",
    "6qoGDc-05wOG",
    "fhlUTwvc53jw",
    "1QF5TgaoUQgz",
    "C969RA6x3zKK"
   ],
   "name": "SImCLR cifar10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "625px",
    "width": "382px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "159070707f784605a2336cffb647d02f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a7b6912ecb3418da2aa22ef34d9ae5a",
      "placeholder": "​",
      "style": "IPY_MODEL_f47647ab20454613b70c4f78c6175c7b",
      "value": " 97.8M/97.8M [46:13&lt;00:00, 37.0kB/s]"
     }
    },
    "1a700b2fc234400498581d2081c89153": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6213f8efc85b41b8b9c23375c416057e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67a457df909442e58782494206d86974",
       "IPY_MODEL_159070707f784605a2336cffb647d02f"
      ],
      "layout": "IPY_MODEL_1a700b2fc234400498581d2081c89153"
     }
    },
    "67a457df909442e58782494206d86974": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3777ffbc9f941e5a52e11a7e2a59ec0",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a38a8031fa3f45feaa930bc12f2efc28",
      "value": 102530333
     }
    },
    "6a7b6912ecb3418da2aa22ef34d9ae5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a38a8031fa3f45feaa930bc12f2efc28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c3777ffbc9f941e5a52e11a7e2a59ec0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f47647ab20454613b70c4f78c6175c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwT1_i5urB-S"
      },
      "source": [
        "# Value Iteration. Policy Iteration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "import numpy as np\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "from gym.envs import toy_text\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "gym.__version__"
      ],
      "metadata": {
        "id": "5tThXctua8EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xtAOEGDsVjjoY13z16gUGNGHoqscSq0m)"
      ],
      "metadata": {
        "id": "D9O3NA8S4pSR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzMNoirrB-c"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeS58CcZrB-c"
      },
      "source": [
        "На лекции мы рассмотрели, как мы можем выучить оптимальную политику, используя алгоритм Value Iteration, если нам известна динамика среды, а также если пространства состояний и действий не большие и дискретные."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1YMBt1zrWJLNUZ6F-He9XVnIHRpYkJXbq)\n"
      ],
      "metadata": {
        "id": "XXMQLYZY46fA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8V198ZWrB-c"
      },
      "source": [
        "Попробуем выучить оптимальную политику в среде <a href=https://gymnasium.farama.org/environments/toy_text/frozen_lake/>FrozenLake-v1</a>. Это простая среда с маленькими пространствами состояний и действий, а также с известной динамикой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1RDsVzIrB-c"
      },
      "source": [
        "Создадим среду и выведем её описание."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dGUx3aVrB-c"
      },
      "source": [
        "env = gym.make('FrozenLake-v1')\n",
        "env.reset()\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWZQhZXMrB-d"
      },
      "source": [
        "print(env.env.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeE7E_RrrB-d"
      },
      "source": [
        "Как видно среда представляет собой поле 4 на 4, по которому нужно добраться от начала (клетка *S*) до цели (клетка *G*). При этом среда является недетерменированный - с определенной вероятностью при совершения действия агент подскользнется и попадет не в ту клетку, в которую направлялся. Клетка *H* обозначает прорубь. Игра закначивается, когда агент попадает в клетку *G* или в клету *H*. Если агент проваливается в прорубь, то он получает награду *0*, если достигает клетки цели *1*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKfH731DrB-d"
      },
      "source": [
        "Посмотрим, сколько в среднем очков награды за 100 эпизодов получит наш агент, если будет выполнять случайные действия."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mebroA_frB-d"
      },
      "source": [
        "env.seed(42);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fandrCwXrB-d"
      },
      "source": [
        "total_reward = []\n",
        "for episode in range(100):\n",
        "    episode_reward = 0\n",
        "    observation = env.reset()\n",
        "    for t in range(100):\n",
        "        # Ваш код здесь\n",
        "\n",
        "        if done:\n",
        "            print(\"Episode {} finished after {} timesteps\".format(episode+1, t+1))\n",
        "            break\n",
        "    total_reward.append(episode_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohnx-1750U37"
      },
      "source": [
        "plt.plot(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJQNMjgrB-e"
      },
      "source": [
        "print(np.mean(total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bkcsr7irB-e"
      },
      "source": [
        "Как видим, только в 2 эпизодах из 100 агену удалось добраться до цели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP2WQJuDrB-e"
      },
      "source": [
        "Из среды OpenAI Gym мы можем получить элементы MDP (Markov Decision Process)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE1bEiMvrB-e"
      },
      "source": [
        "В env.env.P хранится двухуровневый словарь, в котором первый ключ является состояние, а второй - действием.\n",
        "Клетки ассоциированыс индексами [0, 1, 2, ..., 15] слева направо и сверху вниз."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fld4FxfrB-f"
      },
      "source": [
        "Индексы действией [0, 1, 2, 3] соответствуют движению на Запад, Юг, Восток и Север.\n",
        "env.env.P[state][action] возвращает лист кортежей (probability, nextstate, reward). Например, состояние 0 - это начальное состояние и информация о веротностях перехода для s=0 и a=0 содержит:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShWO5jHzrB-f"
      },
      "source": [
        "env.env.P[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRczJz33rB-f"
      },
      "source": [
        "Другой пример - состояние 5 сооветсвует проруби, и все действия в данном состоянии приводят к тому же состоянию с вероятностью 1 и наградой 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6v3d50WrB-f"
      },
      "source": [
        "for i in range(4):\n",
        "    print(\"P[5][%i] =\" % i, env.env.P[5][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulL50EKqrB-g"
      },
      "source": [
        "Вспомним, что из себя представляет алгоритм Value Iteration \n",
        "![](https://drive.google.com/uc?export=view&id=1klIcBCbPCZMp5strcJFpkgGBePYdSNqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpOdbLucrB-g"
      },
      "source": [
        "Задание считается решенным, если агент доходит до цели в среднем в 70% эпизодов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpqiI1NgrB-g"
      },
      "source": [
        "Напишем несклько вспомогательных функций."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhaaGfAtrB-g"
      },
      "source": [
        "Запомним число состояний и действий в среде."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltyUvF-NrB-g"
      },
      "source": [
        "n_states = 16\n",
        "n_actions = 4\n",
        "print(\"Number of states: {}\".format(n_states))\n",
        "print(\"Number of actions: {}\".format(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D17jl3KPrB-h"
      },
      "source": [
        "Поскольку алгоритм Value Iteration возвращает нам оптимальную V-функцию, то нам необходимо извлекать из нее оптимальную политику (как указано в последней строке псевдокода алгоритма)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbflyrGbrB-h"
      },
      "source": [
        "def extract_policy(v, gamma = 1.0):\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGM8knFyrB-h"
      },
      "source": [
        "Также напишем функцию для оценки нашей найденной политики."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APPFY7mjrB-i"
      },
      "source": [
        "def evaluate_policy(env, policy, gamma=1.0, n=100):\n",
        "    total_reward = []\n",
        "    for episode in range(n):\n",
        "        episode_reward = 0\n",
        "        observation = env.reset()\n",
        "        step = 0\n",
        "        for _ in range(100):\n",
        "            # env.render()\n",
        "            action = int(policy[observation])\n",
        "            observation, reward, done, _ = env.step(action)\n",
        "            episode_reward += gamma**step*reward\n",
        "            step += 1\n",
        "            if done:\n",
        "                break\n",
        "        total_reward.append(episode_reward)\n",
        "    return np.mean(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcMS01htrB-i"
      },
      "source": [
        "Нам остается написать основную функцию, которая вернет оптимальную V-функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3n1-T3rB-i"
      },
      "source": [
        "def value_iteration(env, gamma=1.0, max_iterations=100000):\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccGHZkErB-i"
      },
      "source": [
        "Теперь мы можем найти оптимальную V-функцию, извлечь из нее оптимальную политику и оцениь ее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKkg6zzTrB-i"
      },
      "source": [
        "optimal_v = value_iteration(env)\n",
        "optimal_policy = extract_policy(optimal_v)\n",
        "optimal_policy_score = evaluate_policy(env, optimal_policy, n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU7QpdbSrB-j"
      },
      "source": [
        "print(optimal_v.reshape(4,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBhHv_lXrB-j"
      },
      "source": [
        "print(optimal_policy.reshape(4,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYjOOHzlrB-j"
      },
      "source": [
        "print(optimal_policy_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JxyNSmhrB-j"
      },
      "source": [
        "По сравнению со \"случайным\" агентом, который доходил до цели в 3 случаях из 100, наша новая политика позволяет добирться до цели в ~70% эпизодов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL0mKTRWrB-j"
      },
      "source": [
        "## Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFpzQXlTrB-j"
      },
      "source": [
        "Вспомним, что из себя представляет алгоритм Policy Iteration\n",
        "![](https://drive.google.com/uc?export=view&id=1hphERFsRFKpNcBYTSVXgInMXO1WPXKT2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1GnuNFBQ-ns1Bczg1QqgF6Dh6jW_ETf2Y)"
      ],
      "metadata": {
        "id": "LpN03Zd05FOg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z0tz635rB-k"
      },
      "source": [
        "Напишем необходимые вспомогательные функции."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IavoNuEGrB-k"
      },
      "source": [
        "Начнем с основного цикла алгоритма, который вернет нам оптимальную политику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF24dYENrB-k"
      },
      "source": [
        "def policy_iteration(env, gamma=1.0, max_iterations = 200000):\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMGkJ54brB-k"
      },
      "source": [
        "Остается написать 2 функции, которые используются в основном цикле алгоритма Policy Iteration согласно псевдокоду."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoxHkFhSrB-l"
      },
      "source": [
        "def policy_evaluation(env, policy, gamma=1.0, eps=1e-10):\n",
        "    v = np.zeros(n_states)\n",
        "    # Ваш код здесь\n",
        "   \n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnQ_YyXOrB-l"
      },
      "source": [
        "def policy_improvement(v, gamma=1.0):\n",
        "    policy = np.zeros(n_states)\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLO7xE3urB-l"
      },
      "source": [
        "Теперь мы также можем найти оптимальную V-функцию, извлечь из нее оптимальную политику и оцениь ее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TOzVMaMrB-l"
      },
      "source": [
        "optimal_policy = policy_iteration(env)\n",
        "optimal_policy_score = evaluate_policy(env, optimal_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqvQyBPnrB-l"
      },
      "source": [
        "print(optimal_policy.reshape(4,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkOoOmHqrB-l"
      },
      "source": [
        "print(optimal_policy_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1bDuP3J1RcC16P_igo3zLWgpY18h1VgzE)"
      ],
      "metadata": {
        "id": "CV2_47g95Og4"
      }
    }
  ]
}
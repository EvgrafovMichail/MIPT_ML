{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d91707d2-21d1-4504-b9c4-94d4691434ef",
      "metadata": {
        "id": "d91707d2-21d1-4504-b9c4-94d4691434ef"
      },
      "source": [
        "# Семинар № 2 - Сверточные нейронные сети "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbec289-d41e-4e1c-8ff0-812f82277327",
      "metadata": {
        "id": "ecbec289-d41e-4e1c-8ff0-812f82277327"
      },
      "source": [
        "## Requirements "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329cbc82-7da3-4ddb-9ed4-76fddd667cab",
      "metadata": {
        "id": "329cbc82-7da3-4ddb-9ed4-76fddd667cab"
      },
      "source": [
        "```python\n",
        "# core packages\n",
        "!pip install yaml\n",
        "!pip install tqdm\n",
        "!pip insatll scikit-image\n",
        "!pip install opencv-python\n",
        "!pip install opencv-contrib-python\n",
        "!pip install torchinfo\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc7c049-4851-4475-8fd6-a3cefb4fe3e4",
      "metadata": {
        "id": "bfc7c049-4851-4475-8fd6-a3cefb4fe3e4"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "Ee9gI-pdj3hr"
      },
      "id": "Ee9gI-pdj3hr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcee3f44-bdd2-47d7-8f36-3afa1772cf63",
      "metadata": {
        "id": "dcee3f44-bdd2-47d7-8f36-3afa1772cf63"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import Optional\n",
        "\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from sklearn.metrics import f1_score\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d099374-a322-4f08-857e-769463358e77",
      "metadata": {
        "id": "7d099374-a322-4f08-857e-769463358e77"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3d22fd-487a-4c91-abd1-94a9839dd390",
      "metadata": {
        "id": "4a3d22fd-487a-4c91-abd1-94a9839dd390"
      },
      "outputs": [],
      "source": [
        "# fix all seeds\n",
        "seed = 42\n",
        "\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7feb935-6191-476a-8cf6-9dfc444c2436",
      "metadata": {
        "id": "e7feb935-6191-476a-8cf6-9dfc444c2436"
      },
      "source": [
        "# Работа с данными - изображения "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95747a60-397c-4787-9cbf-d53606bc3e73",
      "metadata": {
        "id": "95747a60-397c-4787-9cbf-d53606bc3e73"
      },
      "source": [
        "Загрузим данные из датасета ```CIFAR10``` и реализуем для него ```Dataset``` от торча.\n",
        "\n",
        "Работа с картинками:\n",
        "- предобработка\n",
        "- построение Dataset\n",
        "- аугментации\n",
        "- отображение батчей для проверки преобразований"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFpYWVCxMqJe",
        "outputId": "c3d583ae-98e1-42e9-d4a0-c2be2d16263d"
      },
      "id": "gFpYWVCxMqJe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/Seminar2_02_CNN"
      ],
      "metadata": {
        "id": "huLkOUInM0Rt"
      },
      "id": "huLkOUInM0Rt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Seminar2_02_CNN')"
      ],
      "metadata": {
        "id": "LybUwyAONusU"
      },
      "id": "LybUwyAONusU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993e89a9-b2b7-4995-b471-41ab3d703e09",
      "metadata": {
        "id": "993e89a9-b2b7-4995-b471-41ab3d703e09"
      },
      "outputs": [],
      "source": [
        "from cifar import load_cifar10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c05b903-d590-4bc7-b2d4-4381e679a48d",
      "metadata": {
        "tags": [],
        "id": "9c05b903-d590-4bc7-b2d4-4381e679a48d"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
        "\n",
        "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585279ec-324c-43bc-9fb5-5d3343b07e25",
      "metadata": {
        "id": "585279ec-324c-43bc-9fb5-5d3343b07e25"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[12, 10])\n",
        "for i in range(12):\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "    plt.imshow(np.transpose(X_train[i], [1, 2, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbc4981-3570-4ac7-82ab-6af366468b16",
      "metadata": {
        "id": "dfbc4981-3570-4ac7-82ab-6af366468b16"
      },
      "source": [
        "## Custom Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0625413e-ab9a-42a1-b69e-f11823675b84",
      "metadata": {
        "id": "0625413e-ab9a-42a1-b69e-f11823675b84"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data, classes, transform_augment=None):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        self.cls_names = classes\n",
        "        \n",
        "        self.cls2idx = {name: idx for idx, name in enumerate(self.cls_names)}\n",
        "        self.idx2cls = {idx: name for idx, name in enumerate(self.cls_names)}\n",
        "        \n",
        "        self.transform_augment = transform_augment\n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.x_data) \n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        image = self.x_data[item].transpose(1, 2, 0)\n",
        "        label = self.y_data[item]\n",
        "        \n",
        "        if self.transform_augment is not None:\n",
        "            image = PIL.Image.fromarray((image * 255).astype(np.uint8))\n",
        "            image = self.transform_augment(image)\n",
        "            image = np.array(image)\n",
        "            \n",
        "        if image.max() > 1:\n",
        "            image = image / image.max()\n",
        "            \n",
        "        # image = (image - (0.5, 0.5, 0.5)) / (0.5, 0.5, 0.5)\n",
        "            \n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        image = image.permute(2, 0, 1)  # switch to dim, h, w\n",
        "        \n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        \n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a7948f-48d5-41f6-bb4a-6721420a49d0",
      "metadata": {
        "id": "e9a7948f-48d5-41f6-bb4a-6721420a49d0"
      },
      "outputs": [],
      "source": [
        "transform_augment = transforms.Compose([\n",
        "    transforms.RandomRotation([-30, 30]),\n",
        "    transforms.GaussianBlur(kernel_size=5),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90053570-6068-4884-8992-19d0e5c074f5",
      "metadata": {
        "id": "90053570-6068-4884-8992-19d0e5c074f5"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(X_train, y_train, class_names, transform_augment=transform_augment)\n",
        "valid_dataset = CustomDataset(X_val, y_val, class_names, transform_augment=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7504311-afb4-4d25-915e-4ceedc22a069",
      "metadata": {
        "id": "a7504311-afb4-4d25-915e-4ceedc22a069"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n_workers = 0\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           batch_size=batch_size,       \n",
        "                                           shuffle=True, \n",
        "                                           num_workers=n_workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=n_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aff4fb7-77d3-4dc8-9a6d-80054f82c4c7",
      "metadata": {
        "id": "6aff4fb7-77d3-4dc8-9a6d-80054f82c4c7"
      },
      "source": [
        "## Проверка итераций загрузчика данных "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37404405-e016-46b5-9068-26b2d13c3d97",
      "metadata": {
        "id": "37404405-e016-46b5-9068-26b2d13c3d97"
      },
      "outputs": [],
      "source": [
        "img, label = next(iter(train_loader))\n",
        "print(img.shape, label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed32881f-c52d-4f5a-b8a7-c8f23d807b90",
      "metadata": {
        "id": "ed32881f-c52d-4f5a-b8a7-c8f23d807b90"
      },
      "outputs": [],
      "source": [
        "n_classes = len(train_loader.dataset.cls2idx)\n",
        "print('Количество классов:', n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3643a048-8d07-4456-893d-21d98a653dbf",
      "metadata": {
        "id": "3643a048-8d07-4456-893d-21d98a653dbf"
      },
      "outputs": [],
      "source": [
        "def plot_batch(x):\n",
        "    x = x.permute(0, 2, 3, 1).numpy()\n",
        "        \n",
        "    plt.figure(figsize=[12, 10])\n",
        "    for i in range(12):\n",
        "        plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(x[i])\n",
        "        plt.axis('off')\n",
        "        \n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa85001e-7f0c-4b39-9ab9-1023c264af4c",
      "metadata": {
        "id": "fa85001e-7f0c-4b39-9ab9-1023c264af4c"
      },
      "outputs": [],
      "source": [
        "plot_batch(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ec8687-3735-4692-8cc3-75338e4bc24a",
      "metadata": {
        "tags": [],
        "id": "23ec8687-3735-4692-8cc3-75338e4bc24a"
      },
      "source": [
        "# Построение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3d8918-8681-4feb-aba6-acd90f16476f",
      "metadata": {
        "tags": [],
        "id": "2f3d8918-8681-4feb-aba6-acd90f16476f"
      },
      "source": [
        "## Правила построения тензора\n",
        "\n",
        "В фреймоврке ```pytorch``` размер тензор формируется следующим образом:\n",
        "\n",
        "```python\n",
        "x = torch.ones(array)\n",
        "x.shape\n",
        "# (bs, ch, h, w)\n",
        "```\n",
        "\n",
        "То есть: размер батча, каналы, высота, ширина. Тут речь об изображениях в качества входного тензора."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3e8e50e-137e-4d34-8b1f-d4fd4b4b3777",
      "metadata": {
        "tags": [],
        "id": "c3e8e50e-137e-4d34-8b1f-d4fd4b4b3777"
      },
      "source": [
        "## Сверточные слои"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c2b1d9-6cd8-4444-a527-ad4ffdfe0174",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "b7c2b1d9-6cd8-4444-a527-ad4ffdfe0174"
      },
      "source": [
        "Параметры:\n",
        "\n",
        "| Параметр | Описание |\n",
        "| --- | --- |\n",
        "| in_channels | размер входа |\n",
        "| out_channels | размер выхода |\n",
        "| kernel_size | размер ядра |\n",
        "| --- | --- |\n",
        "| stride | шаг |\n",
        "| padding | отступ |\n",
        "| padding_mode | тип заполнения |\n",
        "| dilation | разреженность |\n",
        "\n",
        "\n",
        "[Convolution animations](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n",
        "\n",
        "\n",
        "```python\n",
        "nn.Conv2d(\n",
        "    in_channels: int,\n",
        "    out_channels: int,\n",
        "    kernel_size: Union[int, Tuple[int, int]],\n",
        "    stride: Union[int, Tuple[int, int]] = 1,\n",
        "    padding: Union[str, int, Tuple[int, int]] = 0,\n",
        "    dilation: Union[int, Tuple[int, int]] = 1,\n",
        "    bias: bool = True,\n",
        "    padding_mode: str = 'zeros',\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1347e66c-55dd-49e6-9c7e-9208dc5ce88b",
      "metadata": {
        "id": "1347e66c-55dd-49e6-9c7e-9208dc5ce88b"
      },
      "outputs": [],
      "source": [
        "bs = 1\n",
        "dim = 3\n",
        "h = 28\n",
        "w = 28\n",
        "x = torch.rand((bs, dim, h, w))\n",
        "\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d36f45b3-32bf-4965-9ad7-486803177c43",
      "metadata": {
        "id": "d36f45b3-32bf-4965-9ad7-486803177c43"
      },
      "outputs": [],
      "source": [
        "simple_block = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=dim,\n",
        "              out_channels=10,\n",
        "              kernel_size=5,\n",
        "              stride=1,\n",
        "              padding=0,\n",
        "              dilation=1,\n",
        "              bias=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d787f23b-8657-458d-afd3-1ca4705832dc",
      "metadata": {
        "id": "d787f23b-8657-458d-afd3-1ca4705832dc"
      },
      "outputs": [],
      "source": [
        "out = simple_block(x)\n",
        "print(x.shape, '->', out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fed1ac-8715-4849-9f35-825797916352",
      "metadata": {
        "id": "a0fed1ac-8715-4849-9f35-825797916352"
      },
      "source": [
        "### Понимаем свертку - поиск паттерна\n",
        "\n",
        "Посмотрим на результат свертки ядра с входным вектором на игрушечном примере. Попытаемся найти паттерн креста на следующей картинке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f83b4d-efc5-4d6b-9d78-c2169393575d",
      "metadata": {
        "id": "75f83b4d-efc5-4d6b-9d78-c2169393575d"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/gdrive/My Drive/Colab Notebooks/Seminar2_02_CNN/cross.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a39bcf-2896-4e7d-b5c6-5648160c3a0f",
      "metadata": {
        "id": "93a39bcf-2896-4e7d-b5c6-5648160c3a0f"
      },
      "outputs": [],
      "source": [
        "img = skimage.io.imread(img_path)\n",
        "\n",
        "plt.imshow(img);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0ccd65-8db7-45ac-a84f-5ab74b5ab898",
      "metadata": {
        "id": "5b0ccd65-8db7-45ac-a84f-5ab74b5ab898"
      },
      "source": [
        "Зададим модель в виде одной свертки с входом по 3 каналам и выходом по 1. Применим такую модель к картинке и посмотрим на результат свертки. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d60164-2f15-489c-96f1-d3b87fb69497",
      "metadata": {
        "id": "d6d60164-2f15-489c-96f1-d3b87fb69497"
      },
      "outputs": [],
      "source": [
        "conv = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(5, 5), bias=False),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b48aa1-f4ad-4937-b9c4-a0a45cba1fda",
      "metadata": {
        "id": "a7b48aa1-f4ad-4937-b9c4-a0a45cba1fda"
      },
      "outputs": [],
      "source": [
        "img_torch = torch.tensor(img / img.max(), dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "img_torch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7402d2ac-c3c1-464c-aa3d-bb5429951d1d",
      "metadata": {
        "id": "7402d2ac-c3c1-464c-aa3d-bb5429951d1d"
      },
      "outputs": [],
      "source": [
        "out = conv(img_torch)\n",
        "\n",
        "out_map = out.squeeze().detach().numpy()\n",
        "out_map = (out_map - out_map.min()) / (out_map.max() - out_map.min())\n",
        "\n",
        "plt.imshow(out_map)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27fb016-89f9-441c-862e-652e93ef6c23",
      "metadata": {
        "id": "c27fb016-89f9-441c-862e-652e93ef6c23"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    m = conv[0].weight[0, i].detach().numpy()\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e52a4f91-cc53-4ebc-8b13-d485137459d7",
      "metadata": {
        "id": "e52a4f91-cc53-4ebc-8b13-d485137459d7"
      },
      "source": [
        "### Зададим веса свертки вручную"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cec376-087b-409e-82ac-60323b9a5602",
      "metadata": {
        "id": "67cec376-087b-409e-82ac-60323b9a5602"
      },
      "outputs": [],
      "source": [
        "simple_cross_conv = np.array([\n",
        "    [0, 0, 1, 0, 0],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [0, 0, 1, 0, 0],\n",
        "])\n",
        "simple_cross_conv = np.repeat(simple_cross_conv[np.newaxis, ...], axis=0, repeats=3)\n",
        "simple_cross_conv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b3dc40-9bf9-4c96-82e0-0246ad1ba26d",
      "metadata": {
        "id": "a2b3dc40-9bf9-4c96-82e0-0246ad1ba26d"
      },
      "outputs": [],
      "source": [
        "conv[0].weight = nn.Parameter(torch.tensor(simple_cross_conv, dtype=torch.float32).unsqueeze(0), requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64098ac6-55ea-4c70-bbb9-904197f922b8",
      "metadata": {
        "id": "64098ac6-55ea-4c70-bbb9-904197f922b8"
      },
      "outputs": [],
      "source": [
        "m = next(iter(conv.parameters()))\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de049048-ff83-4362-8f15-74b8ab4c1ba3",
      "metadata": {
        "id": "de049048-ff83-4362-8f15-74b8ab4c1ba3"
      },
      "outputs": [],
      "source": [
        "out = conv(img_torch)\n",
        "\n",
        "out_map = out.squeeze().detach().numpy()\n",
        "out_map = (out_map - out_map.min()) / (out_map.max() - out_map.min())\n",
        "\n",
        "plt.imshow(out_map)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcac8da4-18d4-47d8-9f8a-5bc4e35670dc",
      "metadata": {
        "id": "fcac8da4-18d4-47d8-9f8a-5bc4e35670dc"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    m = conv[0].weight[0, i].detach().numpy()\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3475238-48a4-4194-b7bc-03776678f6d0",
      "metadata": {
        "id": "c3475238-48a4-4194-b7bc-03776678f6d0"
      },
      "source": [
        "Таким образом, видим, что результат свертки показывает карту, где максимальные значения соотвествуют заданному шаблону. Свертка помогает найти шаблон, заданный в ядре свертки, на входном тензоре."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492f859a-ea2c-40a3-b0fc-1e8be53a9d66",
      "metadata": {
        "tags": [],
        "id": "492f859a-ea2c-40a3-b0fc-1e8be53a9d66"
      },
      "source": [
        "## Пулинг слой"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263468e8-d362-4547-be9b-98a386b6126d",
      "metadata": {
        "id": "263468e8-d362-4547-be9b-98a386b6126d"
      },
      "source": [
        "Преимущество пуллинг слоев состоит в том, что они позволяют сокращать геометрическую размерность тензора. Наращивание слоев нейронной сети позволяет увеличивать размерность векторного представления данных.\n",
        "\n",
        "\n",
        "Параметры:\n",
        "\n",
        "| Параметр | Описание |\n",
        "| --- | --- |\n",
        "| kernel_size | размер ядра |\n",
        "| --- | --- |\n",
        "| stride | шаг |\n",
        "| padding | отступ |\n",
        "| dilation | разреженность |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be31f28d-d790-4f0e-80ab-9ed87b736a2c",
      "metadata": {
        "tags": [],
        "id": "be31f28d-d790-4f0e-80ab-9ed87b736a2c"
      },
      "source": [
        "### Max/Avg Pooling\n",
        "\n",
        "Пулинг в виде свертки. Работает по правилам обычной свертки, как ```nn.Conv2d```.\n",
        "\n",
        "\n",
        "```python\n",
        "nn.MaxPool2d(\n",
        "    kernel_size: Union[int, Tuple[int, ...]],\n",
        "    stride: Union[int, Tuple[int, ...], NoneType] = None,\n",
        "    padding: Union[int, Tuple[int, ...]] = 0,\n",
        "    dilation: Union[int, Tuple[int, ...]] = 1,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822562af-3b10-4291-86a6-0f00bbf7d229",
      "metadata": {
        "id": "822562af-3b10-4291-86a6-0f00bbf7d229"
      },
      "outputs": [],
      "source": [
        "simple_block = nn.Sequential(\n",
        "    nn.MaxPool2d(kernel_size=3,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40069da0-3bf6-49ac-a145-16d44b446209",
      "metadata": {
        "id": "40069da0-3bf6-49ac-a145-16d44b446209"
      },
      "outputs": [],
      "source": [
        "out = simple_block(x)\n",
        "print(x.shape, '->', out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f82f2f-1a10-4e0c-ace1-7622629f9518",
      "metadata": {
        "tags": [],
        "id": "53f82f2f-1a10-4e0c-ace1-7622629f9518"
      },
      "source": [
        "### Adaptive Max/Avg Pooling\n",
        "\n",
        "Адаптивный пулинг. Его преимущество в том, чтобы задать требуемый размер выходного тензора. Это крайне удобно для формирования заданного выхода, не зависящего от размерности входного тензора. Этот прием используется для построения нейронных сетей, которые могут принимать на вход тензоры любого размера. Эту особенность мы далее изучим в ходе реализации архитектур нейронных сетей.\n",
        "\n",
        "\n",
        "```python\n",
        "nn.AdaptiveMaxPool2d(\n",
        "    output_size: Union[int, NoneType, Tuple[Union[int, NoneType], ...]],\n",
        "    return_indices: bool = False,\n",
        ") -> None\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354b8146-d9d3-4be0-b468-a2a2baffd36d",
      "metadata": {
        "id": "354b8146-d9d3-4be0-b468-a2a2baffd36d"
      },
      "outputs": [],
      "source": [
        "simple_block = nn.Sequential(\n",
        "    nn.AdaptiveMaxPool2d(output_size=(10, 10))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e4bc97-7f72-4072-acdc-c5f12b478576",
      "metadata": {
        "id": "02e4bc97-7f72-4072-acdc-c5f12b478576"
      },
      "outputs": [],
      "source": [
        "out = simple_block(x)\n",
        "print(x.shape, '->', out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd11dcf-cdef-4025-a462-66b4f571464b",
      "metadata": {
        "id": "7bd11dcf-cdef-4025-a462-66b4f571464b"
      },
      "outputs": [],
      "source": [
        "x = torch.rand((bs, dim, 512, 512))\n",
        "\n",
        "out = simple_block(x)\n",
        "print(x.shape, '->', out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de579e65-ac76-49a9-bf2b-4be51cf20d1b",
      "metadata": {
        "tags": [],
        "id": "de579e65-ac76-49a9-bf2b-4be51cf20d1b"
      },
      "source": [
        "### Conv + Pooling + ReLU + Linear = CNN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124adfff-349b-4869-9905-b94100b9d856",
      "metadata": {
        "id": "124adfff-349b-4869-9905-b94100b9d856"
      },
      "outputs": [],
      "source": [
        "simple_block = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=dim,\n",
        "              out_channels=10,\n",
        "              kernel_size=5,\n",
        "              stride=1,\n",
        "              padding=0,\n",
        "              dilation=1,\n",
        "              bias=False),\n",
        "    nn.AdaptiveMaxPool2d(output_size=(1, 1)),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=10, out_features=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac942a0-851a-41e9-b40f-a600b2c5f877",
      "metadata": {
        "id": "bac942a0-851a-41e9-b40f-a600b2c5f877"
      },
      "outputs": [],
      "source": [
        "x = torch.rand((bs, dim, h, w))\n",
        "\n",
        "logit = simple_block(x)\n",
        "print(x.shape, '->', logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661dbfad-0a25-4e8d-b2e3-98ec1c485358",
      "metadata": {
        "id": "661dbfad-0a25-4e8d-b2e3-98ec1c485358"
      },
      "outputs": [],
      "source": [
        "prob = torch.softmax(logit, dim=1)  # convert to probabilities\n",
        "prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d676e6f-4f4f-4058-86a2-717d68cf20ec",
      "metadata": {
        "tags": [],
        "id": "4d676e6f-4f4f-4058-86a2-717d68cf20ec"
      },
      "source": [
        "## Нормализация (Batch Norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540ac0e8-acf2-4c0c-9cbf-598c2227664a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "540ac0e8-acf2-4c0c-9cbf-598c2227664a"
      },
      "source": [
        "[Оригинальная статья](https://arxiv.org/abs/1502.03167) - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
        "\n",
        "\n",
        "$y= \\frac{x−E[x]}{\\sqrt{Var[x]+ϵ}}*γ+β$\n",
        "\n",
        "\n",
        "Параметры:\n",
        "\n",
        "| Параметр | Описание |\n",
        "| --- | --- |\n",
        "| num_features | размер входа |\n",
        "| --- | --- |\n",
        "| momentum | параметр вычисления моментов |\n",
        "| affine | обучение | "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57d6f20-e5da-4799-9370-9151a7a75b53",
      "metadata": {
        "id": "f57d6f20-e5da-4799-9370-9151a7a75b53"
      },
      "outputs": [],
      "source": [
        "BatchNorm2d = nn.BatchNorm2d(3)\n",
        "BatchNorm2d = nn.BatchNorm2d(3, affine=False)\n",
        "out = BatchNorm2d(x)\n",
        "\n",
        "print(x.shape, '->', out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a0b5bd-3ca4-4a30-86b2-7ac91a64f0a0",
      "metadata": {
        "tags": [],
        "id": "b0a0b5bd-3ca4-4a30-86b2-7ac91a64f0a0"
      },
      "source": [
        "## Cлои исключения (dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ee7a8f-8902-4c60-a3bf-c1f944de1eb0",
      "metadata": {
        "id": "f3ee7a8f-8902-4c60-a3bf-c1f944de1eb0"
      },
      "source": [
        "[Dropout; Strivastava, Hinton et al.](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
        "\n",
        "\n",
        "Параметры:\n",
        "\n",
        "| Параметр | Описание |\n",
        "| --- | --- |\n",
        "| p | вероятность применения |\n",
        "| inplace | менять ли входное значение |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be05331-3e63-4a2f-9386-e7c63329ca1a",
      "metadata": {
        "id": "0be05331-3e63-4a2f-9386-e7c63329ca1a"
      },
      "outputs": [],
      "source": [
        "Dropout2d = nn.Dropout2d(p=0.2)\n",
        "out = Dropout2d(x)\n",
        "\n",
        "print(x.shape, '->', out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16da7e29-d39d-403b-8d24-4dff5516167b",
      "metadata": {
        "tags": [],
        "id": "16da7e29-d39d-403b-8d24-4dff5516167b"
      },
      "source": [
        "## Построение примитивной модели "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe1cb0a-043a-471e-a4ca-4e54e1e28eda",
      "metadata": {
        "id": "abe1cb0a-043a-471e-a4ca-4e54e1e28eda"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_ch, n_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 16, kernel_size=3, stride=1, padding=0, dilation=1,\n",
        "                               bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, dilation=1,\n",
        "                   bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(output_size=(5, 5)),\n",
        "        )\n",
        "        \n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(5 * 5 * 32, 100),  # output_size * channels\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(100, n_classes, bias=True),\n",
        "        )\n",
        "                \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        out = self.neck(x)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ae5eb3-b5a0-41df-9eaa-6620bebaecc4",
      "metadata": {
        "id": "d7ae5eb3-b5a0-41df-9eaa-6620bebaecc4"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(4 * 8 * 8, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.linear_layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62614248-b68d-4fba-9723-54e680d60999",
      "metadata": {
        "id": "62614248-b68d-4fba-9723-54e680d60999",
        "outputId": "9849c7d0-d754-486c-a510-1bf61015d344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 15])\n"
          ]
        }
      ],
      "source": [
        "model = Net(3, 15)\n",
        "x = torch.ones(2, 3, 32, 32)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0b859a-b8e4-4ec8-8bb0-dc3ed2e05482",
      "metadata": {
        "id": "8d0b859a-b8e4-4ec8-8bb0-dc3ed2e05482"
      },
      "outputs": [],
      "source": [
        "def init_scratch_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b571a38-c5d6-453b-b8c3-a2cfdae8480a",
      "metadata": {
        "id": "2b571a38-c5d6-453b-b8c3-a2cfdae8480a"
      },
      "outputs": [],
      "source": [
        "model.apply(init_scratch_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3ab4b9-4052-449c-8b96-43d54ec82dcf",
      "metadata": {
        "id": "4d3ab4b9-4052-449c-8b96-43d54ec82dcf",
        "tags": []
      },
      "source": [
        "## Отображение архитектуры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167ab42e-9805-49b9-8456-fb55b811305f",
      "metadata": {
        "id": "167ab42e-9805-49b9-8456-fb55b811305f"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac190b0a-33dc-49df-acdd-2681c2e7a06d",
      "metadata": {
        "id": "ac190b0a-33dc-49df-acdd-2681c2e7a06d"
      },
      "outputs": [],
      "source": [
        "batch_size = 12\n",
        "summary(model, input_size=(batch_size, 3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be5a7d9-23e8-42d5-8bf1-a9663427907d",
      "metadata": {
        "tags": [],
        "id": "5be5a7d9-23e8-42d5-8bf1-a9663427907d"
      },
      "source": [
        "# Обучение модели "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99102809-3f4d-47e5-abf0-cbe36715d817",
      "metadata": {
        "id": "99102809-3f4d-47e5-abf0-cbe36715d817"
      },
      "source": [
        "## Настройки обучения\n",
        "\n",
        "Воспользуемся конфигурационными файлами для сохранения информации о конфигурации процесса обучения. Задачи поиска оптимальных параметров обучения для высокой точности сводится к перебору моделей нейронных сетей, подходов к предобработке данных, размеру батча, скорости обучения и так далее. Хоршей практикой является сохранение всех параметров обучения.\n",
        "\n",
        "Одним из способов сохранения параметров являются yaml-файлы. Это прямая аналогия с ```json```, однако yaml имеет преимущества с точки зрения читаемости. В питоне с ними можно работать через пакет ```yaml```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34e01f5-1da6-4f0a-806e-f48aad5ced38",
      "metadata": {
        "id": "e34e01f5-1da6-4f0a-806e-f48aad5ced38"
      },
      "outputs": [],
      "source": [
        "# создадим примитивный словарь гипер-параметров обучения\n",
        "hyp_params = {}\n",
        "\n",
        "hyp_params['batch_size'] = 12\n",
        "hyp_params['lr'] = 0.003\n",
        "hyp_params['epochs'] = 3\n",
        "hyp_params['weight_decay'] = 0.004\n",
        "hyp_params['n_workers'] = 0\n",
        "hyp_params['seed'] = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c3d66a-4947-4adc-9eed-7abcff088ed8",
      "metadata": {
        "id": "23c3d66a-4947-4adc-9eed-7abcff088ed8"
      },
      "outputs": [],
      "source": [
        "# сохраним\n",
        "simple_hyp_params_path = 'test_hyp.yaml'\n",
        "with open(simple_hyp_params_path, 'w') as f:\n",
        "    yaml.dump(hyp_params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e80d8f-65d7-4db9-a3b2-07c57ee38fd0",
      "metadata": {
        "id": "39e80d8f-65d7-4db9-a3b2-07c57ee38fd0"
      },
      "source": [
        "Преимущество yaml-файлов перед json в возможности комментирования внутри файла. За счет этого повышается читаемость метаданных и документации. Таким образом, вы можете сохранять конфигурацию обучения, чтобы анализировать полученные результаты обучения от гиперпараметров.\n",
        "\n",
        "```yaml\n",
        "batch_size: 64 \n",
        "lr: 0.003  # inital learning rate\n",
        "epochs: 3\n",
        "weight_decay: 0.004\n",
        "n_workers: 0\n",
        "\n",
        "seed: 42\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958d0fb6-0868-4f0b-b90e-1529546121a7",
      "metadata": {
        "id": "958d0fb6-0868-4f0b-b90e-1529546121a7"
      },
      "outputs": [],
      "source": [
        "# откроем сохраненный файл с кофигуарцией параметров обучения\n",
        "simple_hyp_params_path = 'test_hyp.yaml'\n",
        "with open(simple_hyp_params_path) as f:\n",
        "    hyp_params = yaml.load(f, Loader=yaml.FullLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf01a78a-eb4f-4abd-9e29-e7939f561b0d",
      "metadata": {
        "id": "bf01a78a-eb4f-4abd-9e29-e7939f561b0d"
      },
      "outputs": [],
      "source": [
        "# отобразим изменения\n",
        "hyp_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a154847e-23be-4d8c-9e97-941e76248561",
      "metadata": {
        "id": "a154847e-23be-4d8c-9e97-941e76248561"
      },
      "outputs": [],
      "source": [
        "# обращение в переменным по ключу - аналогично словарю\n",
        "hyp_params['epochs']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f485d8c-7406-46f2-9e7f-e7d760655e6e",
      "metadata": {
        "id": "3f485d8c-7406-46f2-9e7f-e7d760655e6e"
      },
      "source": [
        "## Примитивный класс обучения "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140d2ff5-88a5-41a7-8bfb-d977d8aa434c",
      "metadata": {
        "id": "140d2ff5-88a5-41a7-8bfb-d977d8aa434c"
      },
      "outputs": [],
      "source": [
        "class BaseTrainProcess:\n",
        "    def __init__(self, hyp):\n",
        "        self.best_loss = 1e100\n",
        "        self.best_acc = 0.0\n",
        "        self.current_epoch = -1\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        self.hyp = hyp\n",
        "\n",
        "        self.lr_scheduler: Optional[torch.optim.lr_scheduler] = None\n",
        "        self.model: Optional[torch.nn.modules] = None\n",
        "        self.optimizer: Optional[torch.optim] = None\n",
        "        self.criterion: Optional[torch.nn.modules] = None\n",
        "        \n",
        "        self.train_loader: Optional[Dataloader] = None\n",
        "        self.valid_loader: Optional[Dataloader] = None\n",
        "        \n",
        "        self.init_params()\n",
        "        \n",
        "    def _init_data(self):\n",
        "                \n",
        "        train_dataset = CustomDataset(X_train, y_train, class_names, transform_augment=None)\n",
        "        valid_dataset = CustomDataset(X_val, y_val, class_names, transform_augment=None)\n",
        "\n",
        "        self.train_loader = DataLoader(train_dataset,\n",
        "                                       batch_size=self.hyp['batch_size'], \n",
        "                                       shuffle=True,\n",
        "                                       num_workers=self.hyp['n_workers'])\n",
        "\n",
        "        self.valid_loader = DataLoader(valid_dataset,\n",
        "                                     batch_size=self.hyp['batch_size'], \n",
        "                                     shuffle=True, \n",
        "                                     num_workers=self.hyp['n_workers'])\n",
        "    \n",
        "    def _init_model(self):\n",
        "        self.model = Net(3, 10)\n",
        "        self.model.apply(init_scratch_weights)\n",
        "        self.model.to(self.device)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hyp['lr'],\n",
        "                                         weight_decay=self.hyp['weight_decay'])\n",
        "\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min')\n",
        "        \n",
        "    def init_params(self): \n",
        "        self._init_data()\n",
        "        self._init_model()\n",
        "    \n",
        "    def save_checkpoint(self, loss_valid, path):\n",
        "        if loss_val[1] >= self.best_acc:\n",
        "            self.best_acc = loss_valid[1]\n",
        "            self.save_model(path)\n",
        "\n",
        "        if loss_val[0] <= self.best_loss:\n",
        "            self.best_loss = loss_valid[0]\n",
        "     \n",
        "    def train_step(self):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        cum_loss = 0.0\n",
        "        cum_acc = 0.0\n",
        "        \n",
        "        proc_loss = 0.0\n",
        "        proc_acc = 0.0\n",
        "        \n",
        "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), \n",
        "                    desc=f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')        \n",
        "        for idx, (images, labels) in pbar:\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "            \n",
        "            with torch.set_grad_enabled(True):\n",
        "                logit = self.model(images)\n",
        "                loss = self.criterion(logit, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.model.zero_grad()\n",
        "                \n",
        "            cur_loss = loss.detach().cpu().numpy()\n",
        "            prob, pred = torch.softmax(logit.detach(), dim=1).topk(k=1)\n",
        "            acc = f1_score(labels, pred, average='macro')\n",
        "            cum_acc += acc\n",
        "    \n",
        "            cum_loss += cur_loss\n",
        "        \n",
        "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
        "            proc_acc = (proc_acc * idx + acc) / (idx + 1)\n",
        "        \n",
        "            s = f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, F1: {proc_acc:4.3f}, BCE: {proc_loss:4.3f}'\n",
        "            pbar.set_description(s)\n",
        "            \n",
        "        cum_loss /= len(self.train_loader)\n",
        "        cum_acc /= len(self.train_loader)\n",
        "        return [cum_loss, cum_acc]\n",
        "    \n",
        "    def valid_step(self):\n",
        "        self.model.eval()\n",
        "        \n",
        "        cum_loss = 0.0\n",
        "        cum_acc = 0.0\n",
        "        \n",
        "        proc_loss = 0.0\n",
        "        proc_acc = 0.0\n",
        "        \n",
        "        pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader), \n",
        "                    desc=f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
        "        for idx, (images, labels) in pbar:\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "            \n",
        "            with torch.set_grad_enabled(False):\n",
        "                logit = self.model(images)\n",
        "                loss = self.criterion(logit, labels)\n",
        "                \n",
        "            cur_loss = loss.detach().cpu().numpy()\n",
        "            prob, pred = torch.softmax(logit.detach(), dim=1).topk(k=1)\n",
        "            acc = f1_score(labels, pred, average='macro')\n",
        "            cum_acc += acc\n",
        "    \n",
        "            cum_loss += cur_loss\n",
        "        \n",
        "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
        "            proc_acc = (proc_acc * idx + acc) / (idx + 1)\n",
        "        \n",
        "            s = f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, F1: {proc_acc:4.3f}, BCE: {proc_loss:4.3f}'\n",
        "            pbar.set_description(s)\n",
        "            \n",
        "        cum_loss /= len(self.valid_loader)\n",
        "        cum_acc /= len(self.valid_loader)\n",
        "        return [cum_loss, cum_acc]\n",
        "    \n",
        "    def run(self):\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        for epoch in range(self.hyp['epochs']):\n",
        "            self.current_epoch = epoch\n",
        "\n",
        "            loss_train = self.train_step()\n",
        "            train_losses.append(loss_train)\n",
        "            self.lr_scheduler.step(loss_train[0])\n",
        "            \n",
        "            loss_valid = self.valid_step()\n",
        "            valid_losses.append(loss_valid)\n",
        "            \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        return train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d7a52a-00d0-46f2-b8b6-3eae7e46cbc6",
      "metadata": {
        "tags": [],
        "id": "14d7a52a-00d0-46f2-b8b6-3eae7e46cbc6"
      },
      "outputs": [],
      "source": [
        "set_seed(hyp_params['seed'])\n",
        "\n",
        "train_process = BaseTrainProcess(hyp=hyp_params)\n",
        "\n",
        "losses = train_process.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8abbf95-70bb-43dc-8ece-23e4b8ea2000",
      "metadata": {
        "id": "f8abbf95-70bb-43dc-8ece-23e4b8ea2000"
      },
      "source": [
        "## Примитивная визуализация результатов обучения "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec118e5-025e-40f0-a419-666b2f67c194",
      "metadata": {
        "id": "3ec118e5-025e-40f0-a419-666b2f67c194"
      },
      "outputs": [],
      "source": [
        "train_losses, valid_losses = losses\n",
        "train_losses = np.array(train_losses).T\n",
        "valid_losess = np.array(valid_losses).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6b5b64-3cf9-4a94-9f62-2a88c99e6c87",
      "metadata": {
        "id": "ff6b5b64-3cf9-4a94-9f62-2a88c99e6c87"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, valid_losess):\n",
        "    \n",
        "    plt.figure(figsize=(6.4 * 2, 4.8 * 1))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plt.plot(train_losses[0, :], color='black', label='train')\n",
        "    plt.plot(valid_losess[0, :], label='valid')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('BCE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.plot(train_losses[1, :], color='black', label='train')\n",
        "    plt.plot(valid_losess[1, :], label='valid')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('F1')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed6f1a3-17ad-47af-89a7-acc073798a06",
      "metadata": {
        "id": "1ed6f1a3-17ad-47af-89a7-acc073798a06"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_losses, valid_losess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a492c9f5-1299-4d31-8697-96b375fbb7f9",
      "metadata": {
        "id": "a492c9f5-1299-4d31-8697-96b375fbb7f9"
      },
      "outputs": [],
      "source": [
        "test_accuracy = np.max(valid_losess[1, :])\n",
        "\n",
        "print(\"Final results:\")\n",
        "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "    test_accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef6d992-35a2-4ad3-b344-4a6d801c8201",
      "metadata": {
        "id": "0ef6d992-35a2-4ad3-b344-4a6d801c8201"
      },
      "source": [
        "## Сохранение лучшей модели \n",
        "\n",
        "Сохранение модели в торче не позволяет сохранить всю архитектуру вместе в весами обученных параметров. То есть загрузка модели происходит в несколько этапов\n",
        "1. Загрузить модель (```nn.Module```), определив все параметры типа количества классов на выходе, размер входа и прочее\n",
        "2. Загрузить веса\n",
        "\n",
        "Для того, чтобы повысить воспроизводимость и интерпретацию результатов, хорошей практикой является сохранять все, что может быть связано с моделью: количество классов, название модели, эксперимента, время создания, размер изображений на обучении, кодировку классов и прочее. Все эти вещи потом помогут в случае, если все запутается. \n",
        "\n",
        "Пример словаря для сохранения:\n",
        "```python\n",
        "save_params = {\n",
        "    'exp_name': args.save_dir.stem,\n",
        "    'model_type': args.model_type,\n",
        "    'encoder_name': args.encoder_name,\n",
        "    'model_state_dict': self.model.state_dict(),\n",
        "    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "    'label_list': tuple(self.cls2idx.keys()),\n",
        "    'num_class': self.data['n_classes'],\n",
        "    'image_size': np.fromstring(self.data['image_size'], sep=',', dtype=int),\n",
        "    'valid_list': self.val_database.data_fname_list,\n",
        "    'ni': self.ni,\n",
        "    'loss': self.best_loss,\n",
        "    'acc': self.best_acc,\n",
        "    'save_time': strftime(\"%Y-%m-%d %H_%M_%S\", gmtime())\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df095dca-dd3a-472e-a31a-dfa804885171",
      "metadata": {
        "id": "df095dca-dd3a-472e-a31a-dfa804885171"
      },
      "outputs": [],
      "source": [
        "from time import gmtime, strftime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dae7640-e560-4a6b-b4d4-ab5958d97fbb",
      "metadata": {
        "id": "6dae7640-e560-4a6b-b4d4-ab5958d97fbb"
      },
      "outputs": [],
      "source": [
        "# создадим примитивную функцию для логирования\n",
        "def save_model(model, path):\n",
        "    save_params = {\n",
        "        'model_state_dict': model.state_dict(),  # веса модели\n",
        "        'save_time': strftime(\"%Y-%m-%d %H_%M_%S\", gmtime())\n",
        "    }\n",
        "    torch.save(save_params, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e770a225-dac6-4932-90f8-4203b37088dd",
      "metadata": {
        "id": "e770a225-dac6-4932-90f8-4203b37088dd"
      },
      "outputs": [],
      "source": [
        "save_model_path = '/content/gdrive/My Drive/Colab Notebooks/Seminar2_02_CNN/checkpoint.pt'\n",
        "\n",
        "save_model(train_process.model, save_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a827d3bf-2c1c-424a-afda-289f81377351",
      "metadata": {
        "id": "a827d3bf-2c1c-424a-afda-289f81377351"
      },
      "source": [
        "Загрузим и проверим, что получилось сохранить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba31b3c7-6a4c-4b33-b4ab-7436f2935473",
      "metadata": {
        "id": "ba31b3c7-6a4c-4b33-b4ab-7436f2935473",
        "outputId": "6d54c58a-8ab1-481f-cf8a-8d414995887b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "save_model_path = '/content/gdrive/My Drive/Colab Notebooks/Seminar2_02_CNN/checkpoint.pt'\n",
        "\n",
        "model = Net(3, 10)\n",
        "\n",
        "# загрузим модель\n",
        "checkpoint = torch.load(save_model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])  # необученная модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2191e0-b9fc-481a-91aa-177ff8512755",
      "metadata": {
        "id": "ae2191e0-b9fc-481a-91aa-177ff8512755"
      },
      "outputs": [],
      "source": [
        "dummy_x = torch.rand((2, 3, 32, 32))\n",
        "\n",
        "model.eval()\n",
        "out_1 = model(dummy_x)\n",
        "\n",
        "train_process.model.to('cpu')\n",
        "train_process.model.eval()\n",
        "out_2 = train_process.model(dummy_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5229f44-a4f2-4567-8d53-fb48d3fc0116",
      "metadata": {
        "id": "b5229f44-a4f2-4567-8d53-fb48d3fc0116"
      },
      "outputs": [],
      "source": [
        "np.isclose(out_1.detach().cpu().numpy(), \n",
        "           out_2.detach().cpu().numpy(),\n",
        "           rtol=1e-5, atol=1e-6).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4fd021-5df9-4858-8857-0e7e9da4a79b",
      "metadata": {
        "id": "fe4fd021-5df9-4858-8857-0e7e9da4a79b"
      },
      "source": [
        "# Литература и ссылки\n",
        "- Документация [PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "- Тензорные операции [cudnn doc](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html)\n",
        "- Convolution arithmetic for deep learning [guide](https://arxiv.org/abs/1603.07285)\n",
        "- NVIDIA Deep Learning Performance [Documentation](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5236c4-1812-4b7c-a173-d022a76206c1",
      "metadata": {
        "id": "fd5236c4-1812-4b7c-a173-d022a76206c1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-autonumbering": true,
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
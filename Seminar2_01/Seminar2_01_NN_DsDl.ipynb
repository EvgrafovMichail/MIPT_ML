{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUF68Fc70QVm"
   },
   "source": [
    "# Семинар 1 - Dataset&Dataloader. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hPVup3XaX7R8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c2k0b73c-H6W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bX7WKpm20QVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThtmhiVeew3d"
   },
   "source": [
    "# PyTorch Dataset and DataLoader [this kernel](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKJhUdPkew3d"
   },
   "source": [
    "## custom Dataset\n",
    "\n",
    "Нам предстоит видоизменять класс Dataset следующего вида\n",
    "\n",
    "official code : \n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "\n",
    "- **__init__()** : initial processes like reading a csv file, assigning transforms, ... \n",
    "- **__len__()** : return the size of input data\n",
    "- **__getitem__()** : return data and label at orbitary index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97bauIxA_BhB"
   },
   "source": [
    "Загрузим исходные данные, которые состоят из набора изображений рукописных цифр размера 28 на 28 пикселей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kj0EL_47foRv"
   },
   "outputs": [],
   "source": [
    "path = \"./train.csv\"\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z4Cyk5QifoUw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GQ2Ok3YSnaCY"
   },
   "outputs": [],
   "source": [
    "image = data.iloc[0, 1:].values.astype(np.uint8).reshape((28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZT5_X-gn_jY",
    "outputId": "d86f1097-3ac4-4414-b0e1-79a6e7305896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kF-3FcJunqAP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22ba0c61a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM90lEQVR4nO3df6zddX3H8derPyhJC7NX6KUrnSBrljUmFnNTnTWOSSRAshQTMVZD6kK8Rm1WnMsg7A/ZfwxBptvE1NFRjcKMQuiSRm0qGXEQwm3t2mIdsFq1P9I76B8U0fa2fe+P+2W5lns+53LO95zv6X0/H8nJOef7Pt/zfeekr37O+X7OuR9HhADMfnOabgBAfxB2IAnCDiRB2IEkCDuQxLx+HuwCL4gLtbCfhwRS+a1+rVNx0tPVugq77eslfVnSXEn/EhF3lx5/oRbq3b62m0MCKHgmdrSsdfw23vZcSf8s6QZJKyWts72y0+cD0FvdfGZfLenFiDgQEackPSJpbT1tAahbN2FfJulXU+4fqrb9Dtujtsdsj03oZBeHA9CNbsI+3UmAN3z3NiI2RcRIRIzM14IuDgegG92E/ZCk5VPuXy7pSHftAOiVbsL+rKQVtq+0fYGkj0raWk9bAOrW8dRbRJy2vUHSDzQ59bY5Ip6rrTMAtepqnj0itknaVlMvAHqIr8sCSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kERfl2wG+mnxfw61rD1y5Y+K+77z7z9TrF/25ac66qlJjOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATz7DhvDT99cbH+1eWtFxieiPnFfR0dtTTQugq77YOSTkg6I+l0RIzU0RSA+tUxsv9ZRLxUw/MA6CE+swNJdBv2kPRD2zttj073ANujtsdsj03oZJeHA9Cpbt/Gr4mII7aXSNpu+2cR8eTUB0TEJkmbJOliD83C0x7A+aGrkT0ijlTX45Iek7S6jqYA1K/jsNteaPui129Luk7SvroaA1Cvbt7GD0t6zPbrz/PtiPh+LV0Bkg7c8yfF+iOX31esL/CClrX37FpX3Pf3HyqPW2eK1cHUcdgj4oCkd9bYC4AeYuoNSIKwA0kQdiAJwg4kQdiBJPiJKxpz/C/KU2tPr7u3WF8058Ji/Ysvr2xZG/5E+bdbZ155pVg/HzGyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASzLOjp+b+0R+2rK393BPFfX+vzTz6nlPlH5o+fu8HWtbe8vLTxX1nI0Z2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCeXZ0ZeK68sK9H7jvP1rW/mroZ10d+5P3bCzWL/1Gvrn0EkZ2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCeXYUHfvL9xbrO2//p2L9rKJl7fmJU8V9b/3pLcX60scOFOuni9V82o7stjfbHre9b8q2Idvbbb9QXS/ubZsAujWTt/EPSbr+nG13SNoRESsk7ajuAxhgbcMeEU9KOn7O5rWStlS3t0i6qd62ANSt0xN0wxFxVJKq6yWtHmh71PaY7bEJnezwcAC61fOz8RGxKSJGImJkvhb0+nAAWug07MdsL5Wk6nq8vpYA9EKnYd8qaX11e72kx+tpB0CvtJ1nt/2wpGskXWL7kKQvSLpb0nds3yrpl5Ju7mWT6J15V/xBsf7x0R/07Ng3j32yWF/+4X3FOvPob07bsEfEuhala2vuBUAP8XVZIAnCDiRB2IEkCDuQBGEHkuAnrrPc3OGW32SWJL3/3/cX67ctfr7NEVys/vz0b1vWFm67qM1zo06M7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBPPss93Fi4rlbpdNbue2d/15y9rQyyyp3E+M7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBPPss8C8y5e1rK3+bnkefU6b36O387mj7y7W4zetf8+O/mJkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkmGefBca/trBl7c5L9hb3PdvmuTceWVOs//xPy+PF2ddea3ME9Evbkd32ZtvjtvdN2XaX7cO2d1eXG3vbJoBuzeRt/EOSrp9m+/0Rsaq6bKu3LQB1axv2iHhS0vE+9AKgh7o5QbfB9p7qbf7iVg+yPWp7zPbYhE52cTgA3eg07A9IukrSKklHJd3X6oERsSkiRiJiZL4WdHg4AN3qKOwRcSwizkTEWUlfl7S63rYA1K2jsNteOuXuhyTta/VYAIOh7Ty77YclXSPpEtuHJH1B0jW2V0kKSQclfap3LaL0e3VJ+uCyzv/2+6tny+dRdn7l6mL9La/xt9/PF23DHhHrptn8YA96AdBDfF0WSIKwA0kQdiAJwg4kQdiBJPiJ6wCY97blxfpF3/51sf53S37SsvbSmd8U973h3r8p1oe/+VSxjvMHIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME8+wD4xbryPPtPrvjHjp/79sPlP/w7/BXm0bNgZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJJhn74Pxz7y3WH/0019s8wwXFqsbDr+vZe3ljw+1ee5X2tQxWzCyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASzLPXYO6llxbrf73x34r1K+eV59Hb2fXAqpa1oQMsqYxJbUd228ttP2F7v+3nbG+stg/Z3m77hep6ce/bBdCpmbyNPy3p8xHxx5LeI+mztldKukPSjohYIWlHdR/AgGob9og4GhG7qtsnJO2XtEzSWklbqodtkXRTj3oEUIM3dYLO9hWSrpb0jKThiDgqTf6HIGlJi31GbY/ZHpvQyS7bBdCpGYfd9iJJ35N0W0TM+NcTEbEpIkYiYmS+FnTSI4AazCjstudrMujfiohHq83HbC+t6ksljfemRQB1aDv1ZtuSHpS0PyK+NKW0VdJ6SXdX14/3pMPzwOGPrSjWP7Lo+z09/qmL3dPnx+wwk3n2NZJukbTX9u5q252aDPl3bN8q6ZeSbu5JhwBq0TbsEfFjSa2GjmvrbQdAr/B1WSAJwg4kQdiBJAg7kARhB5LgJ641mDNRrk/EmWJ9vucW6yejfIATV7V+/suKeyITRnYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJ59hos+epTxfq/briqWF84p/znuu7/2oeL9RX/UD4+IDGyA2kQdiAJwg4kQdiBJAg7kARhB5Ig7EASzLP3wdaVb+1q/8vEPDq6x8gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0m0Dbvt5bafsL3f9nO2N1bb77J92Pbu6nJj79sF0KmZfKnmtKTPR8Qu2xdJ2ml7e1W7PyLu7V17AOoyk/XZj0o6Wt0+YXu/pGW9bgxAvd7UZ3bbV0i6WtIz1aYNtvfY3mx7cYt9Rm2P2R6bUPnPLwHonRmH3fYiSd+TdFtEvCLpAUlXSVqlyZH/vun2i4hNETESESPztaD7jgF0ZEZhtz1fk0H/VkQ8KkkRcSwizkTEWUlfl7S6d20C6NZMzsZb0oOS9kfEl6ZsXzrlYR+StK/+9gDUZSZn49dIukXSXtu7q213Slpne5WkkHRQ0qd60B+AmszkbPyPJXma0rb62wHQK3yDDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIQjon8Hs/9X0i+mbLpE0kt9a+DNGdTeBrUvid46VWdvb4uIS6cr9DXsbzi4PRYRI401UDCovQ1qXxK9dapfvfE2HkiCsANJNB32TQ0fv2RQexvUviR661Rfemv0MzuA/ml6ZAfQJ4QdSKKRsNu+3vZ/237R9h1N9NCK7YO291bLUI813Mtm2+O2903ZNmR7u+0Xqutp19hrqLeBWMa7sMx4o69d08uf9/0zu+25kp6X9EFJhyQ9K2ldRPy0r420YPugpJGIaPwLGLbfL+lVSd+IiHdU2+6RdDwi7q7+o1wcEbcPSG93SXq16WW8q9WKlk5dZlzSTZI+oQZfu0JfH1EfXrcmRvbVkl6MiAMRcUrSI5LWNtDHwIuIJyUdP2fzWklbqttbNPmPpe9a9DYQIuJoROyqbp+Q9Poy442+doW++qKJsC+T9Ksp9w9psNZ7D0k/tL3T9mjTzUxjOCKOSpP/eCQtabifc7VdxrufzllmfGBeu06WP+9WE2GfbimpQZr/WxMR75J0g6TPVm9XMTMzWsa7X6ZZZnwgdLr8ebeaCPshScun3L9c0pEG+phWRByprsclPabBW4r62Osr6FbX4w338/8GaRnv6ZYZ1wC8dk0uf95E2J+VtML2lbYvkPRRSVsb6OMNbC+sTpzI9kJJ12nwlqLeKml9dXu9pMcb7OV3DMoy3q2WGVfDr13jy59HRN8vkm7U5Bn5/5H0t0300KKvt0v6r+ryXNO9SXpYk2/rJjT5juhWSW+VtEPSC9X10AD19k1JeyXt0WSwljbU2/s0+dFwj6Td1eXGpl+7Ql99ed34uiyQBN+gA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk/g+/DMuBLxBsJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4dkTQTipEop",
    "outputId": "e372e9b8-0f0f-4b7d-ac91-17d042dbe5dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = data.iloc[0, 0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8QchA6YnqDl",
    "outputId": "49078b08-1ecd-4d38-a2f2-834e940a16b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t8ejtAIew3d"
   },
   "source": [
    "Изучив данные, приступим к модификации класса Dataset. Нам предстоит видеоизменить 2 функции - длину объекта (len) и итератор (getitem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyjRsG3uew3d"
   },
   "outputs": [],
   "source": [
    "class DatasetMNIST(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = # ваш код\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # взять изображение типа ndarray type (Height * Width * Channels)\n",
    "        # сконвертировать тип в np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # преобразовать numpy ndarray в tensor in PyTorch при следующем размере (H, W, C) --> (C, H, W)\n",
    "\n",
    "        # ваш код\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9BrCphbew3e"
   },
   "outputs": [],
   "source": [
    "train_dataset = DatasetMNIST(path, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrHjS9RooNcp",
    "outputId": "c8f6c612-845f-4fcf-d090-5494a5be60af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DatasetMNIST at 0x7f1c1a7feeb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJRD08eOew3e"
   },
   "source": [
    "## transform is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyEBqC7Zew3e"
   },
   "outputs": [],
   "source": [
    "# возьмем данные по итератору объекта (__getitem__(index))\n",
    "img, lab = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxYLKcJTew3e",
    "outputId": "09e1bfdd-3e61-4251-a88a-9f6d9d651ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkM9bVGyew3e"
   },
   "source": [
    "## take a look at the dataset\n",
    "\n",
    "теперь мы можем объявить класс DataLoader - итератор нашего класса Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toB_EUobew3e"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1587fk7oew3e"
   },
   "source": [
    "Мы можем использовать загрузчик данных в качестве итератора, используя функцию iter() и next()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJqG4gPyew3e",
    "outputId": "37727e30-19e0-4a1b-a511-f2fc37935c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SO7bkigew3e",
    "outputId": "77a2308c-bb01-4e94-e7f6-bf7f481d94fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape on batch size = torch.Size([8, 1, 28, 28])\n",
      "labels shape on batch size = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(train_iter)\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIo13sNFew3e"
   },
   "outputs": [],
   "source": [
    "# сделаем сетку из изображений\n",
    "# tensor : (batchsize, channels, height, width)\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKSb_yXcew3e"
   },
   "source": [
    "## transform it ToTensor()\n",
    "\n",
    "Добавим нативные преобразования для изображений-тензоров в pytorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wujgJUFNew3e"
   },
   "outputs": [],
   "source": [
    "class DatasetMNIST2(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = # ваш код\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # загрузить изображение как тип ndarray (Высота * Ширина * Каналы)\n",
    "        # будьте осторожны при преобразовании dtype в np.uint8 [Целое число без знака (от 0 до 255)]\n",
    "        # в этом примере мы используем ToTensor(), поэтому определяем массив numpy следующим образом (H, W, C)\n",
    "        \n",
    "        # ваш код\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_Ous8U-ew3e"
   },
   "outputs": [],
   "source": [
    "train_dataset2 = DatasetMNIST2(path, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97jZ6EPAew3e",
    "outputId": "852469c6-b5a5-4dce-8a3c-062a2909367a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape at the first row : torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "img, lab = train_dataset2.__getitem__(0)\n",
    "\n",
    "print('image shape at the first row : {}'.format(img.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T85b2TwKew3e"
   },
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(train_dataset2, batch_size=8, shuffle=True)\n",
    "\n",
    "train_iter2 = iter(train_loader2)\n",
    "print(type(train_iter2))\n",
    "\n",
    "images, labels = next(train_iter2)\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "an440dr2ew3e"
   },
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3JMMiW3ew3e"
   },
   "source": [
    "## transform it [ToTensor(), some [augmentations](https://pytorch.org/docs/stable/torchvision/transforms.html)]\n",
    "\n",
    "Преобразования -  методы используют некоторый тип входных данных, таких как (только тензор), (тензор или numpy), (только изображение PIL), поэтому вы должны учитывать порядок преобразования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YETZNHE7ew3e"
   },
   "source": [
    "**ToTensor()**\n",
    "\n",
    "```python\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    (this is only for np.uint8 type)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "To Tensor() принимает **PIL-изображение** или **numpy ndarray** (обе формы (высота, ширина, каналы))\n",
    "\n",
    "**ToPILImage**\n",
    "\n",
    "```python\n",
    "    \"\"\"Convert a tensor or an ndarray to PIL Image.\n",
    "    Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n",
    "    H x W x C to a PIL Image while preserving the value range.\n",
    "    Args:\n",
    "        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n",
    "            If ``mode`` is ``None`` (default) there are some assumptions made about the input data:\n",
    "            1. If the input has 3 channels, the ``mode`` is assumed to be ``RGB``.\n",
    "            2. If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``.\n",
    "            3. If the input has 1 channel, the ``mode`` is determined by the data type (i,e,\n",
    "            ``int``, ``float``, ``short``).\n",
    "    .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Для загрузки изображения() требуется **torch.*Tensor ( C, H, W )** или **numpy ndarray ( H, W, C )**\n",
    "\n",
    "**RandomHorizontalFlip**\n",
    "\n",
    "```python\n",
    "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "RandomHorizontalFlip() использует только **PIL Image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fRUm0Cvew3e"
   },
   "source": [
    "Если вы хотите использовать несколько преобразований, вы должны составить список, используя **torchvision.transforms.Compose**\n",
    "\n",
    "Эта функция может преобразовать некоторое изображение по порядку в рамках метода **\\__call__**.\n",
    "\n",
    "```python\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlokXrORew3e"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "  # ваш код\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNePCTZKew3e"
   },
   "outputs": [],
   "source": [
    "train_dataset3 = DatasetMNIST2(path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaHjUe-sew3e"
   },
   "outputs": [],
   "source": [
    "train_loader3 = DataLoader(train_dataset3, batch_size=8, shuffle=True)\n",
    "\n",
    "train_iter3 = iter(train_loader3)\n",
    "print(type(train_iter3))\n",
    "\n",
    "images, labels = next(train_iter3)\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GaLWiXRew3e"
   },
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Zd3xUJDCX_J"
   },
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JARapdz_GfIZ"
   },
   "source": [
    "## Слои можно [определить самостоятельно](https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77)\n",
    "\n",
    "но зачем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SxzcjwcGiVK"
   },
   "outputs": [],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        bias = torch.Tensor(size_out)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x= torch.mm(x, self.weights.t())\n",
    "        return torch.add(w_times_x, self.bias)  # w times x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1jWry_bGmIL"
   },
   "outputs": [],
   "source": [
    "linear = MyLinearLayer(256, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnZsVNiZIBUN"
   },
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZEQWkQbDRBA"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fl = torch.nn.Flatten(start_dim=1)\n",
    "        self.fc1 = torch.nn.Linear(784, 128)\n",
    "        self.act1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "        self.act2 = torch.nn.Sigmoid()\n",
    "        self.fc3 = torch.nn.Linear(10, 10)\n",
    "        self.act3 = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fl(x)\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys2k7w-XHdMt"
   },
   "source": [
    "## [Batchnorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tZwenDb-Hiu0"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fl = torch.nn.Flatten(start_dim=1)\n",
    "        self.fc1 = torch.nn.Linear(784, 128)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.Sigmoid()\n",
    "        self.drp = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "        self.act2 = torch.nn.Sigmoid()\n",
    "        self.fc3 = torch.nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fl(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)        \n",
    "        x = self.act1(x)\n",
    "        x = self.drp(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTGIi80xAgx8",
    "outputId": "7989fced-5714-4b8f-fee7-6fb7371adbf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fl): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): Sigmoid()\n",
       "  (drp): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (act2): Sigmoid()\n",
       "  (fc3): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe6U_87SvUzq"
   },
   "source": [
    "## Обучение с DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "edkyX5zY3xzL"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "Y = data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2oEgb8bs5MfE"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.7, random_state=1)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j7aDdNQl3SrA"
   },
   "outputs": [],
   "source": [
    "class DatasetMNIST(Dataset):\n",
    "    \n",
    "    def __init__(self, x: pd.DataFrame, y: pd.DataFrame, transform=None, train: bool = True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # загрузить изображение как тип ndarray (Высота * Ширина * Каналы)\n",
    "        # будьте осторожны при преобразовании dtype в np.uint8 [Целое число без знака (от 0 до 255)]\n",
    "        # в этом примере мы используем ToTensor(), поэтому определяем массив numpy следующим образом (H, W, C)\n",
    "        image = self.x.iloc[index, :].values.astype(np.uint8).reshape((28, 28, 1))\n",
    "        label = self.y.iloc[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, label) if self.train else image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9Qgvmlj_vaaS"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2312233274.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [18]\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_dataset = # ваш код\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DatasetMNIST(x_train, y_train)\n",
    "val_dataset = DatasetMNIST(x_val, y_val)\n",
    "test_dataset = DatasetMNIST(x_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9coPzIU6t15",
    "outputId": "585b16bb-0a54-4163-865f-32cae76fd15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "img, lab = train_dataset.__getitem__(0)\n",
    "print(img.shape)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d6_CJvd2wi-"
   },
   "outputs": [],
   "source": [
    "train_loader = # ваш код\n",
    "val_loader = # ваш код\n",
    "test_loader = # ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkljrSJgHKd3"
   },
   "outputs": [],
   "source": [
    "# определим модель\n",
    "\n",
    "model = Model()\n",
    "model.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAhvzRfo8H0x"
   },
   "outputs": [],
   "source": [
    "# определим алгоритм оптимизации и функцию ошибки\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sgKQYsBCcT7",
    "outputId": "b886eb42-7b63-4d23-febf-782b44f438a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим устройство для обучения\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv-14B4_3HF8"
   },
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "loss_train_ep = []\n",
    "loss_train_step = []\n",
    "grads = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = 0\n",
    "    for x, y in train_loader:\n",
    "        optim.zero_grad()\n",
    "        x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "        y = torch.tensor(y, device=device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_step = loss.detach().cpu().numpy()\n",
    "        loss_epoch += loss_step\n",
    "        loss_train_step = np.append(loss_train_step, loss_step)\n",
    "    \n",
    "    # *посчитаем градиенты\n",
    "    for params in model.parameters():\n",
    "        print(params.grad())\n",
    "  \n",
    "  # посчитаем точность на валидации\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    for x, y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y, device=device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            accuracy += (predicted == y).sum().item()\n",
    "\n",
    "    local_acc = accuracy / total / len(val_loader)\n",
    "    print(f'Точность {local_acc:.2f}')\n",
    "\n",
    "    loss_train_ep = np.append(loss_train_ep, loss_epoch)\n",
    "\n",
    "print(loss_train_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mwjHAJZ-EY3"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.plot(loss_train_ep, label='loss_train_ep')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EWkbgm7S-Hs"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.plot(grads, label='grads')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7rBBgidD_HI"
   },
   "source": [
    "## Лернинг рейт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ipxc5WGqG3u3"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "lrs = []\n",
    "epoches = 30\n",
    "for i in range(epoches):\n",
    "    lr = 0.001*(1/( 1 + np.exp(1/epoches * (i*10-epoches))))\n",
    "    lrs.append(lr)\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4C46qJQRpBN"
   },
   "outputs": [],
   "source": [
    "def cyclical_lr(stepsize, epochs, min_lr=3e-4, max_lr=3e-3):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1.\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)*2**(-it//(2*stepsize))\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = np.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqXWmk4kRkQf"
   },
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "step_size = epochs/20\n",
    "clr = cyclical_lr(step_size, epochs, min_lr=1e-9, max_lr=1e-1)\n",
    "lr = []\n",
    "for id in range(epochs):\n",
    "  # print(i)\n",
    "  lr.append(clr(id))\n",
    "plt.plot(lr[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCjuImMdTR7k"
   },
   "source": [
    " [Cosine annealing](https://paperswithcode.com/method/cosine-annealing#:~:text=Cosine%20Annealing%20is%20a%20type,before%20being%20increased%20rapidly%20again.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UIu_kzxDZ4J"
   },
   "source": [
    "## Использование предобученных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHmLSgCeVaI6"
   },
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvWQMoLsDZSP"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dia0QxwUDsOv"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSOLwKGoVfIa"
   },
   "outputs": [],
   "source": [
    "def change_layers(model):\n",
    "    model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.fc = torch.nn.Linear(2048, 10, bias=True)\n",
    "    return model\n",
    "\n",
    "change_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA6Hwpn7DVpw"
   },
   "source": [
    "### Отображение архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13mymJQADpBO"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 256\n",
    "summary(model, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrP-wpF6XL7w"
   },
   "source": [
    "### Дообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbdHZ9TzXLTZ"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2Wk4xccfiHQ"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YYsRKlLxXLeR"
   },
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "loss_train_ep = []\n",
    "loss_train_step = []\n",
    "grads = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = 0\n",
    "    for x, y in train_loader:\n",
    "        optim.zero_grad()\n",
    "        x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "        y = torch.tensor(y, device=device)\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        loss_step = loss.detach().cpu().numpy()\n",
    "        loss_epoch += loss_step\n",
    "        loss_train_step = np.append(loss_train_step, loss_step)\n",
    "        \n",
    "        # посчитаем градиенты\n",
    "        # ваш код\n",
    "  \n",
    "    # посчитаем точность на валидации\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    for x, y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y, device=device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            accuracy += (predicted == y).sum().item()\n",
    "    \n",
    "    local_acc = accuracy / total / len(val_loader)\n",
    "    print(f'Точность {local_acc:.2f}')\n",
    "\n",
    "    loss_train_ep = np.append(loss_train_ep, loss_epoch)\n",
    "\n",
    "print(loss_train_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-pxaJ5NdXoVq"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.plot(loss_train_ep, label='loss_train_ep')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fWG9h8_kXoVr"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.plot(grads, label='grads')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjVM5MpuKnpJ"
   },
   "source": [
    "# Сохранение и загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSqLFWDDKnpK"
   },
   "source": [
    "#### Сохранение и... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SdhunATDKnpK"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 02 - NN/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZuzzHqiKnpM"
   },
   "source": [
    "#### ... загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTkki5OlKnpN",
    "outputId": "68ada2f5-4823-4ea5-c704-482682120e5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим модель\n",
    "loaded_model = models.resnet50(pretrained=False)\n",
    "change_layers(loaded_model)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load('./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 02 - NN/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq9LHwmPKnpP"
   },
   "outputs": [],
   "source": [
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvcoVamvZD_4",
    "outputId": "87ba980a-a345-404d-ae16-088b3e7d27b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9265079365079365\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "total = 0\n",
    "for x, y in val_loader:\n",
    "  with torch.no_grad():\n",
    "    x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "    y = torch.tensor(y, device=device)\n",
    "    outputs = model(x)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y.size(0)\n",
    "    accuracy += (predicted == y).sum().item()\n",
    "print(accuracy/total/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5bq2617DRB6"
   },
   "source": [
    "# Ссылки\n",
    "\n",
    "1. [Cosine Annealing](https://paperswithcode.com/method/cosine-annealing#:~:text=Cosine%20Annealing%20is%20a%20type,before%20being%20increased%20rapidly%20again.)\n",
    "\n",
    "2. [A Visual Guide to Learning Rate Schedulers in PyTorch](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)\n",
    "\n",
    "3. [BatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "\n",
    "4. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "\n",
    "5. [Torch DATASETS & DATALOADERS](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

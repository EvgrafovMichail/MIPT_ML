{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKuuu7rcyYjD"
   },
   "source": [
    "# Семинар № 5 - Распознавание объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P28KqUbyYjG"
   },
   "source": [
    "# 1.EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sc0D9M0ryYjH"
   },
   "source": [
    "### Установите и импортируйте необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24134,
     "status": "ok",
     "timestamp": 1647955865995,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "aHjBw63Jyq_Z",
    "outputId": "d30a922b-113f-40b7-8060-27bed7f14216",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install albumentations\n",
    "# !pip install opencv-contrib-python\n",
    "# !pip install \"opencv-python-headless<4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "executionInfo": {
     "elapsed": 3122,
     "status": "ok",
     "timestamp": 1647955869114,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "QWMClV5syYjH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1647955869115,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "MbWon6wuyYjI"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1647955869115,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "p9CtjYqhyYjI"
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcBBnt23yYjK"
   },
   "source": [
    "## [Загрузить](https://disk.yandex.ru/d/zwlQ0xbBygL58Q) обучающий и тестовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1647955278314,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "iU3USv5yyYjK"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"global-wheat-detection/train.csv\")\n",
    "submit = pd.read_csv(\"global-wheat-detection/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA7BQqlPyYjL"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FK7HRmEiyYjL"
   },
   "outputs": [],
   "source": [
    "# Удалите ненужные столбцы\n",
    "train_df = train_df.drop(columns=['width','height','source']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2jDQspHyYjL"
   },
   "outputs": [],
   "source": [
    "# В обучающем наборе данных всего 3373 уникальных изображения\n",
    "train_df['image_id'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfnsC8GhyYjL"
   },
   "outputs": [],
   "source": [
    "# максимальное количество полей в одном изображении - 116\n",
    "(train_df['image_id'].value_counts()).max()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_s9VcvXyYjM"
   },
   "outputs": [],
   "source": [
    "# Минимальное количество блоков в одном изображении равно 1\n",
    "(train_df['image_id'].value_counts()).min() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIcv7kusyYjM"
   },
   "source": [
    "### Разделение размера блока в формате [xmin, ymin, w, h]\n",
    "#### Позже мы преобразуем определение box в [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHxTWVD1yYjM"
   },
   "outputs": [],
   "source": [
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGG57mpayYjM"
   },
   "outputs": [],
   "source": [
    "train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x))) ##Lets convert the Box in \n",
    "train_df['x'] = train_df['x'].astype(np.float32)                                        #in our desired formate    \n",
    "train_df['y'] = train_df['y'].astype(np.float32)\n",
    "train_df['w'] = train_df['w'].astype(np.float32)\n",
    "train_df['h'] = train_df['h'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmjpq1S5yYjN"
   },
   "outputs": [],
   "source": [
    "train_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4j2sWccyYjN"
   },
   "outputs": [],
   "source": [
    "submit[['x', 'y', 'w', 'h']] = np.stack(submit['PredictionString'].apply(lambda x: [0, 0, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxYyUbmAyYjN"
   },
   "outputs": [],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YK6C_9_VyYjN"
   },
   "source": [
    "### Разделение данных на обучающий и валидационный наборы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnzHvswEyYjN"
   },
   "outputs": [],
   "source": [
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMbfN4cwyYjO"
   },
   "source": [
    "# 2.Написание пользовательского набора данных для нашей работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVNo9HXlyYjO"
   },
   "source": [
    "### 2.1 Написание пользовательского набора данных для обучающих и валидационных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k--f6bacyYjO"
   },
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None,train=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        # чтение изображения\n",
    "        image = None\n",
    "        \n",
    "        # чтение данных (bbox) - x1, y1, x2, y2\n",
    "        records = self.df[self.df['image_id'] == image_id]   \n",
    "        boxes = None\n",
    "\n",
    "        # чтение класса\n",
    "        labels = None\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "        # преобразование\n",
    "        if self.transforms:\n",
    "            None\n",
    "            \n",
    "        return image, target, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnXkVw-pyYjO"
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(),\n",
    "                A.GaussNoise(var_limit=(0, 0.1)),\n",
    "            ], p=0.7),\n",
    "            A.OneOf([\n",
    "                A.Transpose(),\n",
    "                A.RandomRotate90(),\n",
    "                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "                                   rotate_limit=45,\n",
    "                                   border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "                A.NoOp()\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                           contrast_limit=0.2, p=0.8),\n",
    "                A.RandomGamma(gamma_limit=(70, 130)),\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2,\n",
    "                                     val_shift_limit=0.2, p=0.6),\n",
    "                A.NoOp()\n",
    "            ], p=0.8),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            A.Cutout(num_holes=12, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YKLzuXcyYjO"
   },
   "outputs": [],
   "source": [
    "train_dir = 'global-wheat-detection/train'\n",
    "test_dir = 'global-wheat-detection/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-wFs3h8yYjO"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = WheatDataset(train_df, train_dir, get_train_transforms(), True)\n",
    "valid_dataset = WheatDataset(valid_df, train_dir, get_valid_transforms(), True)\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMA4SVO8yYjP"
   },
   "source": [
    "### Давайте визуализируем некоторые изображения с помощью ограничивающей рамки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zr8I5Lr1yYjP"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPr9u799yYjP"
   },
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "idx = 2\n",
    "boxes = targets[idx]['boxes'].cpu().numpy().astype(int)\n",
    "sample = images[idx].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    sample = cv2.rectangle(sample.copy(), (box[0], box[1]), (box[2], box[3]), (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaov3rHtyYjP"
   },
   "source": [
    "# 3.Точная настройка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHrnWAnbyYjP"
   },
   "source": [
    "### Определение модели\n",
    "\n",
    "Faster R-CNN - это модель, которая предсказывает как ограничительные рамки, так и оценки классов для потенциальных объектов на изображении.\n",
    "\n",
    "\n",
    "Давайте объясним, как работает эта архитектура, поскольку RCNN состоит из 3 частей\n",
    "\n",
    "1. Часть 1: Слои свертки: Архитектура CNN формируется стеком отдельных слоев, которые преобразуют входной объем в выходной объем (например, хранящий оценки класса) с помощью дифференцируемой функции.Сверточные сети были вдохновлены биологическими процессами в том смысле, что структура связей между нейронами напоминает организацию зрительной коры головного мозга животных. Отдельные нейроны коры головного мозга реагируют на стимулы только в ограниченной области поля зрения, известной как рецептивное поле. Рецептивные поля разных нейронов частично перекрываются таким образом, что они охватывают все поле зрения.\n",
    "\n",
    "2. Часть 2: Сеть предложения региона (RPN): RPN - это небольшая нейронная сеть, скользящая по последней карте объектов слоев свертки и предсказывающая, есть объект или нет, а также предсказывающая ограничивающую рамку этих объектов.\n",
    "\n",
    "3. Часть 3: Предсказание классов и ограничивающих рамок: Теперь мы используем другую полностью связанную нейронную сеть, которая принимает в качестве inpt области, предложенные RPN, и предсказывает класс объекта (классификация) и ограничивающие рамки (регрессия)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmAIl3dFyYjR"
   },
   "outputs": [],
   "source": [
    "# загрузить модель; предварительно обученную на COCO\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True, pretrained_backbone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8laxHogyYjS"
   },
   "outputs": [],
   "source": [
    "# класс № 1 (пшеница) + фон\n",
    "num_classes = 2  \n",
    "\n",
    "# получить количество входных объектов для классификатора\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# замените предварительно подготовленную головку на новую\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:      ##Return the average loss \n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qwd6k79syYjS"
   },
   "source": [
    "### Давайте потренируем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v_1QZ9zyYjS"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in tqdm(train_data_loader):\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values()).type(torch.float32)\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knPz1brkyYjS"
   },
   "source": [
    "# 4. Предсказание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK48k_TAyYjS"
   },
   "source": [
    "### Давайте загрузим тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZMHkGrOyYjS"
   },
   "outputs": [],
   "source": [
    "test_dataset = WheatDataset(submit, test_dir, get_valid_transforms(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olAWpoVUyYjS"
   },
   "outputs": [],
   "source": [
    "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyhG2ZnkyYjT"
   },
   "source": [
    "### Установите пороговое значение для прогнозирования ограничивающей рамки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxeVaAaxyYjT"
   },
   "outputs": [],
   "source": [
    "detection_threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI0lM2sPyYjT"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "model.eval()\n",
    "\n",
    "for images, _, image_ids in tqdm(test_data_loader):    \n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()    ##Formate of the output's box is [Xmin,Ymin,Xmax,Ymax]\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32) #Compare the score of output with the threshold and\n",
    "        scores = scores[scores >= detection_threshold]                    #slelect only those boxes whose score is greater\n",
    "                                                                          # than threshold value\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]         #Convert the box formate to [Xmin,Ymin,W,H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjRvZpoQyYjT"
   },
   "outputs": [],
   "source": [
    "sample = images[1].permute(1, 2, 0).cpu().numpy()\n",
    "boxes = outputs[1]['boxes'].data.cpu().numpy()\n",
    "scores = outputs[1]['scores'].data.cpu().numpy()\n",
    "\n",
    "boxes = boxes[scores >= detection_threshold].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJYMAG8YyYjT"
   },
   "source": [
    "### Давайте построим некоторые из наших прогнозов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_p1CMe2yYjT"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    sample = cv2.rectangle(sample.copy(),\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 2)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylCLRsbhyYjT"
   },
   "source": [
    "## Bonus - AP\n",
    "\n",
    "Расчет метрики для задачи распознавания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a: np.array, b: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Args\n",
    "        a: (N, 4) ndarray of float [xmin, ymin, xmax, ymax]\n",
    "        b: (K, 4) ndarray of float [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    Returns\n",
    "        overlaps: (N, K) ndarray of overlap between boxes a and boxes b\n",
    "    \"\"\"\n",
    "    a_area = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1])\n",
    "    b_area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    dx = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], axis=1), b[:, 0])\n",
    "    dy = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], axis=1), b[:, 1])\n",
    "\n",
    "    intersection = np.maximum(dx, 0) * np.maximum(dy, 0)\n",
    "    union = np.expand_dims(a_area, axis=1) + b_area - intersection\n",
    "    overlaps = intersection / union\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_res(inference_res, iou_threshold=0.5, score_threshold=0.05):\n",
    "    \"\"\" Evaluate a given dataset using a given model.\n",
    "    # Arguments\n",
    "        inference_res   : inference results for whole imageset List((target,prediction)),\n",
    "            where targets {'boxes':np.array[4,n], 'labels':np.array[n]},\n",
    "            prediction {'boxes':np.array[4,n], 'labels':np.array[n], scores: np.array[n]}\n",
    "            example:\n",
    "\n",
    "            [({'boxes': np.array([[1321.8750,  274.6667, 1348.8750,  312.6667]]),\n",
    "                'labels': np.array([1])},\n",
    "              {'boxes': np.array([[1323.5446,  275.2711, 1350.2203,  315.9069],\n",
    "                        [ 119.2671, 1227.5459,  171.1528, 1277.9830],\n",
    "                        [ 240.5078, 1147.3656,  270.7879, 1205.0126],\n",
    "                        [ 140.9097, 1231.9814,  173.9967, 1285.4724]]),\n",
    "                'scores': np.array([0.9568, 0.3488, 0.1418, 0.0771]),\n",
    "                'labels': np.array([1, 1, 1, 1])}),\n",
    "             ({'boxes': np.array([[ 798.7500, 1357.3334,  837.7500, 1396.6666],\n",
    "                        [ 829.1250,  777.3333,  873.3750,  818.0000],\n",
    "                        [ 886.5000,   34.6667,  916.5000,   77.3333]]),\n",
    "                'labels': np.array([1, 1, 1])},\n",
    "              {'boxes': np.array([[ 796.5808, 1354.9255,  836.5349, 1395.8972],\n",
    "                        [ 828.8597,  777.9426,  872.5923,  819.8660],\n",
    "                        [ 887.7839,   37.1435,  914.8092,   76.3933]]),\n",
    "                'scores': np.array([0.9452, 0.8701, 0.8424]),\n",
    "                'labels': np.array([1, 1, 1])})]\n",
    "\n",
    "        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "        score_threshold : The score confidence threshold to use for detections.\n",
    "    \"\"\"\n",
    "    false_positives = np.zeros((0,))\n",
    "    true_positives = np.zeros((0,))\n",
    "    scores = np.zeros((0,))\n",
    "    num_annotations = sum([t['labels'].shape[0] for t, _ in inference_res])\n",
    "\n",
    "    for i, (annotations, detections) in enumerate(inference_res):\n",
    "        # detections = p\n",
    "        # annotations = t\n",
    "        detected_annotations = []\n",
    "\n",
    "        if annotations['labels'].shape[0] == 0:  # no objects was there\n",
    "            false_positives = np.append(false_positives, np.ones(detections['labels'].shape[0]))\n",
    "            true_positives = np.append(true_positives, np.zeros(detections['labels'].shape[0]))\n",
    "            continue\n",
    "\n",
    "        for d in np.arange(detections['labels'].shape[0]):\n",
    "            if detections['scores'][d] > score_threshold:\n",
    "                scores = np.append(scores, detections['scores'][d])\n",
    "\n",
    "                overlaps = compute_overlap(np.expand_dims(detections['boxes'][d].astype(np.double), axis=0),\n",
    "                                           annotations['boxes'].astype(np.double))\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap = overlaps[0, assigned_annotation][0]\n",
    "\n",
    "                true_label = annotations['labels'][assigned_annotation][0]\n",
    "                predict_label = detections['labels'][d]\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations and true_label == predict_label:\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives = np.append(true_positives, 0)\n",
    "\n",
    "    # F1@IoU\n",
    "    plain_recall = np.sum(true_positives) / np.fmax(num_annotations, np.finfo(np.float64).eps)\n",
    "    plain_precision = np.sum(true_positives) / np.fmax(np.sum(true_positives) + np.sum(false_positives),\n",
    "                                                       np.finfo(np.float64).eps)\n",
    "    F1 = 2 * plain_precision * plain_recall / np.fmax(plain_precision + plain_recall,\n",
    "                                                      np.finfo(np.float64).eps)\n",
    "\n",
    "    # compute false positives and true positives\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    false_positives = np.cumsum(false_positives[indices])\n",
    "    true_positives = np.cumsum(true_positives[indices])\n",
    "    # compute recall and precision\n",
    "    recall = true_positives / num_annotations\n",
    "    precision = true_positives / np.fmax(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "    # compute average precision\n",
    "    average_precision = compute_ap(recall, precision)\n",
    "\n",
    "    return average_precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OIcv7kusyYjM",
    "YK6C_9_VyYjN",
    "UVNo9HXlyYjO",
    "ZMA4SVO8yYjP",
    "VHrnWAnbyYjP",
    "Qwd6k79syYjS",
    "GK48k_TAyYjS",
    "NyhG2ZnkyYjT",
    "aJYMAG8YyYjT"
   ],
   "name": "seminar_07_solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
